[
    {
        "title": "Finetuning Llama2 7B on Personal Dataset with an IITian | ML/LLM Project",
        "description": "#ai #llm #finetuning #project \n\nThis discusses attention layer, transformers breakdown via code and supervised finetuning Llama model on personal dataset of any kind - each line in GREAT DETAIL.\n\nProject notebook: https://colab.research.google.com/drive/1oP4rHcjVsOJZocMBeJRuKFlhz3LkGIZ6?usp=sharing\nDataset: Self made\n\nFinetuning on Huggingface dataset - https://youtu.be/dn2anUU0d0U\nBuild youtube video summarizer with LLM - https://youtu.be/xDQL3vWwcp0?si=adFk3VReKlR7OHhn\nAIR 12 hackathon challenge using finetuning - https://youtu.be/F-0Gzb2GbxI?si=Jwenow0ZrnvDcWIy",
        "availableLangs": [
            "en"
        ],
        "lengthInSeconds": "3354",
        "thumbnails": [
            {
                "url": "https://i.ytimg.com/vi/OxfeK423y2I/default.jpg",
                "width": 120,
                "height": 90
            },
            {
                "url": "https://i.ytimg.com/vi/OxfeK423y2I/mqdefault.jpg",
                "width": 320,
                "height": 180
            },
            {
                "url": "https://i.ytimg.com/vi/OxfeK423y2I/hqdefault.jpg",
                "width": 480,
                "height": 360
            },
            {
                "url": "https://i.ytimg.com/vi/OxfeK423y2I/sddefault.jpg",
                "width": 640,
                "height": 480
            },
            {
                "url": "https://i.ytimg.com/vi/OxfeK423y2I/hq720.jpg?sqp=-oaymwEcCK4FEIIDSEbyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLD6qlnIUnot0MTmR_D6xGkBCH2S7A",
                "width": 686,
                "height": 386
            }
        ],
        "transcription": [
            {
                "subtitle": "So this tokenized train data set is",
                "start": 0.4,
                "dur": 3.36
            },
            {
                "subtitle": "this. So this is very important. This is",
                "start": 2.24,
                "dur": 4.16
            },
            {
                "subtitle": "how we are going to make the our our own",
                "start": 3.76,
                "dur": 4.159
            },
            {
                "subtitle": "data set which is a bunch of text files",
                "start": 6.4,
                "dur": 3.92
            },
            {
                "subtitle": "random text files into a format that can",
                "start": 7.919,
                "dur": 4.161
            },
            {
                "subtitle": "be trained that can be fine- tuned on",
                "start": 10.32,
                "dur": 4.16
            },
            {
                "subtitle": "the model. So it has the last three",
                "start": 12.08,
                "dur": 3.92
            },
            {
                "subtitle": "layers the gate projection the up",
                "start": 14.48,
                "dur": 2.879
            },
            {
                "subtitle": "position and the down prediction. So",
                "start": 16,
                "dur": 3.359
            },
            {
                "subtitle": "gate projection. Hello everyone welcome",
                "start": 17.359,
                "dur": 3.601
            },
            {
                "subtitle": "back to my channel. Today we are diving",
                "start": 19.359,
                "dur": 3.361
            },
            {
                "subtitle": "into something really exciting",
                "start": 20.96,
                "dur": 3.6
            },
            {
                "subtitle": "fine-tuning llama 2 model which is one",
                "start": 22.72,
                "dur": 3.44
            },
            {
                "subtitle": "of the most powerful opensource language",
                "start": 24.56,
                "dur": 3.76
            },
            {
                "subtitle": "models out there. But the cool part is",
                "start": 26.16,
                "dur": 4.32
            },
            {
                "subtitle": "that we going to do it on our personal",
                "start": 28.32,
                "dur": 3.68
            },
            {
                "subtitle": "main data set which is a bunch of bunch",
                "start": 30.48,
                "dur": 3.52
            },
            {
                "subtitle": "of text files that you can find anywhere",
                "start": 32,
                "dur": 3.36
            },
            {
                "subtitle": "on the internet or you can generate",
                "start": 34,
                "dur": 3.44
            },
            {
                "subtitle": "yourself. We&#39;ll be using some clever",
                "start": 35.36,
                "dur": 3.6
            },
            {
                "subtitle": "techniques like LoRa which basically",
                "start": 37.44,
                "dur": 3.36
            },
            {
                "subtitle": "lets us train just a tiny bit of the",
                "start": 38.96,
                "dur": 3.84
            },
            {
                "subtitle": "model&#39;s parameters while keeping all its",
                "start": 40.8,
                "dur": 3.919
            },
            {
                "subtitle": "original knowledge. Think of it like",
                "start": 42.8,
                "dur": 4.16
            },
            {
                "subtitle": "teaching uh an expert something without",
                "start": 44.719,
                "dur": 3.52
            },
            {
                "subtitle": "making them forget everything they",
                "start": 46.96,
                "dur": 3.759
            },
            {
                "subtitle": "already know. I&#39;ll be using the Hawaiian",
                "start": 48.239,
                "dur": 4.16
            },
            {
                "subtitle": "wildfire data set as our example, but",
                "start": 50.719,
                "dur": 2.961
            },
            {
                "subtitle": "the techniques I&#39;ll be showing you today",
                "start": 52.399,
                "dur": 2.881
            },
            {
                "subtitle": "can be applied to pretty much any data",
                "start": 53.68,
                "dur": 3.6
            },
            {
                "subtitle": "set you want to work with. Try out on",
                "start": 55.28,
                "dur": 4.079
            },
            {
                "subtitle": "anything you want to work with CSV, be",
                "start": 57.28,
                "dur": 5.36
            },
            {
                "subtitle": "it tables, be it Excel files, be it",
                "start": 59.359,
                "dur": 5.12
            },
            {
                "subtitle": "PowerPoint presentations, be it word",
                "start": 62.64,
                "dur": 3.519
            },
            {
                "subtitle": "documents, whatever you want to, you can",
                "start": 64.479,
                "dur": 3.521
            },
            {
                "subtitle": "do it. And we&#39;ll be using 4bit",
                "start": 66.159,
                "dur": 3.441
            },
            {
                "subtitle": "quantization to preserve the model&#39;s",
                "start": 68,
                "dur": 3.04
            },
            {
                "subtitle": "memory footprint and you can run it in",
                "start": 69.6,
                "dur": 3.68
            },
            {
                "subtitle": "collab very very easily. Okay, since I",
                "start": 71.04,
                "dur": 3.28
            },
            {
                "subtitle": "don&#39;t want to waste your time, here&#39;s",
                "start": 73.28,
                "dur": 3.519
            },
            {
                "subtitle": "the agenda for today. So let&#39;s dive",
                "start": 74.32,
                "dur": 3.24
            },
            {
                "subtitle": "right",
                "start": 76.799,
                "dur": 3.36
            },
            {
                "subtitle": "in. Okay, so the first thing is that we",
                "start": 77.56,
                "dur": 4.44
            },
            {
                "subtitle": "are going to download all the install",
                "start": 80.159,
                "dur": 3.841
            },
            {
                "subtitle": "all the data sets.",
                "start": 82,
                "dur": 3.28
            },
            {
                "subtitle": "So the first thing is that we&#39;re going",
                "start": 84,
                "dur": 2.64
            },
            {
                "subtitle": "to install all the libraries that we&#39;ll",
                "start": 85.28,
                "dur": 4.4
            },
            {
                "subtitle": "need today. So let&#39;s do that. And also I",
                "start": 86.64,
                "dur": 5.28
            },
            {
                "subtitle": "would like to say before we start just",
                "start": 89.68,
                "dur": 5.6
            },
            {
                "subtitle": "connect here and to change runtime type",
                "start": 91.92,
                "dur": 5.519
            },
            {
                "subtitle": "to make it T4 GPU because we&#39;re going to",
                "start": 95.28,
                "dur": 6.76
            },
            {
                "subtitle": "need GPU for finetuning definitely. Um",
                "start": 97.439,
                "dur": 8.081
            },
            {
                "subtitle": "yeah let&#39;s do reconnect. Sounds good.",
                "start": 102.04,
                "dur": 5
            },
            {
                "subtitle": "Now the next thing is let&#39;s install all",
                "start": 105.52,
                "dur": 5.639
            },
            {
                "subtitle": "the libraries that we&#39;ll need today.",
                "start": 107.04,
                "dur": 4.119
            },
            {
                "subtitle": "We&#39;re going to need PFT. We are going to",
                "start": 119.92,
                "dur": 4.799
            },
            {
                "subtitle": "need accelerate. And I&#39;ll tell you why",
                "start": 122.64,
                "dur": 4.64
            },
            {
                "subtitle": "we&#39;ll need each of them. Just give me a",
                "start": 124.719,
                "dur": 6.76
            },
            {
                "subtitle": "moment. We&#39;ll need",
                "start": 127.28,
                "dur": 4.199
            },
            {
                "subtitle": "um bits",
                "start": 131.72,
                "dur": 6.96
            },
            {
                "subtitle": "and byes. We are going to",
                "start": 134.76,
                "dur": 5.6
            },
            {
                "subtitle": "need",
                "start": 138.68,
                "dur": 3.88
            },
            {
                "subtitle": "transformers and we are going to need",
                "start": 140.36,
                "dur": 4.519
            },
            {
                "subtitle": "data",
                "start": 142.56,
                "dur": 2.319
            },
            {
                "subtitle": "sets. Okay. So let&#39;s let&#39;s discuss why",
                "start": 147,
                "dur": 3.959
            },
            {
                "subtitle": "we&#39;ll need each of these libraries in",
                "start": 149.44,
                "dur": 3.24
            },
            {
                "subtitle": "detail while these are",
                "start": 150.959,
                "dur": 5.041
            },
            {
                "subtitle": "loading. So PFT what is PFT? PFT is",
                "start": 152.68,
                "dur": 5.4
            },
            {
                "subtitle": "short form for parameter efficient",
                "start": 156,
                "dur": 3.92
            },
            {
                "subtitle": "finetuning. So why is it needed? Because",
                "start": 158.08,
                "dur": 3.36
            },
            {
                "subtitle": "it will be needed to implement the lower",
                "start": 159.92,
                "dur": 3.599
            },
            {
                "subtitle": "or the low rank adaptation. So what is",
                "start": 161.44,
                "dur": 3.28
            },
            {
                "subtitle": "that? Basically, it allows us to",
                "start": 163.519,
                "dur": 3.201
            },
            {
                "subtitle": "fine-tune large models like llama 2",
                "start": 164.72,
                "dur": 3.84
            },
            {
                "subtitle": "which has so many billions of parameters",
                "start": 166.72,
                "dur": 4.4
            },
            {
                "subtitle": "without updating all the parameters. And",
                "start": 168.56,
                "dur": 4.88
            },
            {
                "subtitle": "basically, we&#39;ll use Lora config and get",
                "start": 171.12,
                "dur": 5.199
            },
            {
                "subtitle": "PFG model parameters using the PFT which",
                "start": 173.44,
                "dur": 4.72
            },
            {
                "subtitle": "will help us to fine-tune just a few",
                "start": 176.319,
                "dur": 5.84
            },
            {
                "subtitle": "lurank matrices of this entire um 7",
                "start": 178.16,
                "dur": 5.84
            },
            {
                "subtitle": "billion or 3 billion parameters that we",
                "start": 182.159,
                "dur": 4.401
            },
            {
                "subtitle": "have in total. I I&#39;ll get into more",
                "start": 184,
                "dur": 5.04
            },
            {
                "subtitle": "detail as we&#39;re using the function. So,",
                "start": 186.56,
                "dur": 5.12
            },
            {
                "subtitle": "hold your horses. Next one is pip",
                "start": 189.04,
                "dur": 4.08
            },
            {
                "subtitle": "install aselate. So why do we need",
                "start": 191.68,
                "dur": 3.279
            },
            {
                "subtitle": "accelerate? Because for optimization and",
                "start": 193.12,
                "dur": 3.44
            },
            {
                "subtitle": "hardware utilization. These are all very",
                "start": 194.959,
                "dur": 3.2
            },
            {
                "subtitle": "important because you are actually you",
                "start": 196.56,
                "dur": 3.52
            },
            {
                "subtitle": "have to remember that you are training a",
                "start": 198.159,
                "dur": 4.08
            },
            {
                "subtitle": "billion parameter model. Remember it&#39;s a",
                "start": 200.08,
                "dur": 4.72
            },
            {
                "subtitle": "billion parameter model on these like",
                "start": 202.239,
                "dur": 5.521
            },
            {
                "subtitle": "GPUs and uh across different kinds of",
                "start": 204.8,
                "dur": 5.04
            },
            {
                "subtitle": "implementations on your laptop or on",
                "start": 207.76,
                "dur": 3.52
            },
            {
                "subtitle": "Google Collab whatever you&#39;re doing. So",
                "start": 209.84,
                "dur": 2.959
            },
            {
                "subtitle": "you need to be as much efficient as",
                "start": 211.28,
                "dur": 2.959
            },
            {
                "subtitle": "possible because you don&#39;t want to waste",
                "start": 212.799,
                "dur": 3.121
            },
            {
                "subtitle": "time. You don&#39;t want to lose memory. You",
                "start": 214.239,
                "dur": 3.041
            },
            {
                "subtitle": "want to save everything. You don&#39;t want",
                "start": 215.92,
                "dur": 3.2
            },
            {
                "subtitle": "to spend so much money without getting",
                "start": 217.28,
                "dur": 4.8
            },
            {
                "subtitle": "enough you know back. That is why it&#39;s",
                "start": 219.12,
                "dur": 5.039
            },
            {
                "subtitle": "very important. So this accelerated",
                "start": 222.08,
                "dur": 4
            },
            {
                "subtitle": "helps to distribute training across GPUs",
                "start": 224.159,
                "dur": 4.08
            },
            {
                "subtitle": "and it implements sort of like a",
                "start": 226.08,
                "dur": 4.4
            },
            {
                "subtitle": "precision training. So basically it",
                "start": 228.239,
                "dur": 4.241
            },
            {
                "subtitle": "speeds up the processing significantly.",
                "start": 230.48,
                "dur": 3.52
            },
            {
                "subtitle": "You don&#39;t need to know so much details",
                "start": 232.48,
                "dur": 4.24
            },
            {
                "subtitle": "but overall a basic idea would be good.",
                "start": 234,
                "dur": 5.36
            },
            {
                "subtitle": "Next thing is the pip install bits bits",
                "start": 236.72,
                "dur": 5.519
            },
            {
                "subtitle": "and byes. So bits and byes is going to",
                "start": 239.36,
                "dur": 4.64
            },
            {
                "subtitle": "be sort of like the four bit",
                "start": 242.239,
                "dur": 5.2
            },
            {
                "subtitle": "quantization. we going to use",
                "start": 244,
                "dur": 3.439
            },
            {
                "subtitle": "that. Okay. So the next thing is the",
                "start": 252.599,
                "dur": 5.001
            },
            {
                "subtitle": "transformers. So definitely transformers",
                "start": 255.68,
                "dur": 3.519
            },
            {
                "subtitle": "is the main library for transformer",
                "start": 257.6,
                "dur": 2.879
            },
            {
                "subtitle": "models. We&#39;re going to import auto",
                "start": 259.199,
                "dur": 2.88
            },
            {
                "subtitle": "tokenizer. We&#39;re going to import auto",
                "start": 260.479,
                "dur": 3.681
            },
            {
                "subtitle": "model for causal LM and we&#39;re going to",
                "start": 262.079,
                "dur": 4.081
            },
            {
                "subtitle": "use llama tokenizer trainer model which",
                "start": 264.16,
                "dur": 3.599
            },
            {
                "subtitle": "are which is basically used for SFT",
                "start": 266.16,
                "dur": 2.96
            },
            {
                "subtitle": "training. We&#39;re going to do SF training",
                "start": 267.759,
                "dur": 2.88
            },
            {
                "subtitle": "which is basically supervised fine",
                "start": 269.12,
                "dur": 3.44
            },
            {
                "subtitle": "tuning. What do I mean by that? So it",
                "start": 270.639,
                "dur": 3.201
            },
            {
                "subtitle": "basically means that you&#39;re going to",
                "start": 272.56,
                "dur": 2.88
            },
            {
                "subtitle": "have a data set where you know what",
                "start": 273.84,
                "dur": 3.68
            },
            {
                "subtitle": "you&#39;re going to give the output as. For",
                "start": 275.44,
                "dur": 4.16
            },
            {
                "subtitle": "example, let&#39;s say we have an instruct",
                "start": 277.52,
                "dur": 4.8
            },
            {
                "subtitle": "model um like a chat model. We know a",
                "start": 279.6,
                "dur": 4.08
            },
            {
                "subtitle": "question, we know an answer. So we&#39;re",
                "start": 282.32,
                "dur": 2.72
            },
            {
                "subtitle": "going to provide data set in the form of",
                "start": 283.68,
                "dur": 3.76
            },
            {
                "subtitle": "a question answer format, Q&amp;A format or",
                "start": 285.04,
                "dur": 3.68
            },
            {
                "subtitle": "we&#39;re going to provide it like a text",
                "start": 287.44,
                "dur": 3.36
            },
            {
                "subtitle": "document format and then convert it what",
                "start": 288.72,
                "dur": 3.36
            },
            {
                "subtitle": "we&#39;re going to do today. So, we&#39;re going",
                "start": 290.8,
                "dur": 3.679
            },
            {
                "subtitle": "to give it a bunch of text documents and",
                "start": 292.08,
                "dur": 4.96
            },
            {
                "subtitle": "these are all on like Hawaiian wildfire",
                "start": 294.479,
                "dur": 4
            },
            {
                "subtitle": "what I found on the internet like",
                "start": 297.04,
                "dur": 3.12
            },
            {
                "subtitle": "randomly but you can use it for anything",
                "start": 298.479,
                "dur": 5.28
            },
            {
                "subtitle": "you want and maybe some kind of bot some",
                "start": 300.16,
                "dur": 5.759
            },
            {
                "subtitle": "kind of agents some kind of I don&#39;t know",
                "start": 303.759,
                "dur": 4.641
            },
            {
                "subtitle": "like some kind of HR questions interview",
                "start": 305.919,
                "dur": 4.161
            },
            {
                "subtitle": "questions whatever like text documents",
                "start": 308.4,
                "dur": 3.04
            },
            {
                "subtitle": "and you can make a model which is",
                "start": 310.08,
                "dur": 2.959
            },
            {
                "subtitle": "basically fine-tuned on that task so",
                "start": 311.44,
                "dur": 4
            },
            {
                "subtitle": "that it can answer on the documents text",
                "start": 313.039,
                "dur": 4.641
            },
            {
                "subtitle": "documents perfectly. So that is what is",
                "start": 315.44,
                "dur": 4.88
            },
            {
                "subtitle": "the purpose of today&#39;s video and this is",
                "start": 317.68,
                "dur": 4.079
            },
            {
                "subtitle": "what you&#39;re going to use it as these",
                "start": 320.32,
                "dur": 3.52
            },
            {
                "subtitle": "training this trainer model can be found",
                "start": 321.759,
                "dur": 4.88
            },
            {
                "subtitle": "from the transformers library. Cool. Now",
                "start": 323.84,
                "dur": 4.24
            },
            {
                "subtitle": "next thing comes the data set. This is",
                "start": 326.639,
                "dur": 2.641
            },
            {
                "subtitle": "very simple. We&#39;re going to use the load",
                "start": 328.08,
                "dur": 2.8
            },
            {
                "subtitle": "data set model and we&#39;re going to do",
                "start": 329.28,
                "dur": 4.56
            },
            {
                "subtitle": "kind of data set efficient loading and",
                "start": 330.88,
                "dur": 4.72
            },
            {
                "subtitle": "processing pre-processing of data set to",
                "start": 333.84,
                "dur": 3.6
            },
            {
                "subtitle": "make it in the way we want for our model",
                "start": 335.6,
                "dur": 4.319
            },
            {
                "subtitle": "to train on. So these packages they work",
                "start": 337.44,
                "dur": 4
            },
            {
                "subtitle": "together to enable the efficient",
                "start": 339.919,
                "dur": 3.681
            },
            {
                "subtitle": "fine-tuning of large language models on",
                "start": 341.44,
                "dur": 4.319
            },
            {
                "subtitle": "basically limited hardware resources. uh",
                "start": 343.6,
                "dur": 3.92
            },
            {
                "subtitle": "which is exactly what our code is doing",
                "start": 345.759,
                "dur": 3.44
            },
            {
                "subtitle": "because we don&#39;t have that much many",
                "start": 347.52,
                "dur": 3.6
            },
            {
                "subtitle": "resources many GPUs and everything we",
                "start": 349.199,
                "dur": 3.521
            },
            {
                "subtitle": "have limited resources so we going to",
                "start": 351.12,
                "dur": 4.799
            },
            {
                "subtitle": "make the maximum use out of",
                "start": 352.72,
                "dur": 3.199
            },
            {
                "subtitle": "them great so this is done so now next",
                "start": 356.12,
                "dur": 5.44
            },
            {
                "subtitle": "thing we&#39;re going to need",
                "start": 360.16,
                "dur": 3.879
            },
            {
                "subtitle": "is pip",
                "start": 361.56,
                "dur": 6
            },
            {
                "subtitle": "install GPU",
                "start": 364.039,
                "dur": 6.201
            },
            {
                "subtitle": "util so what does this do so this",
                "start": 367.56,
                "dur": 4.919
            },
            {
                "subtitle": "basically does something like the GPU",
                "start": 370.24,
                "dur": 4.16
            },
            {
                "subtitle": "utilization you will be able to suppose",
                "start": 372.479,
                "dur": 3.201
            },
            {
                "subtitle": "you&#39;re not using collab because in",
                "start": 374.4,
                "dur": 2.799
            },
            {
                "subtitle": "collab we have only one GPU which is a",
                "start": 375.68,
                "dur": 3.68
            },
            {
                "subtitle": "T4 GPU but suppose you are using your",
                "start": 377.199,
                "dur": 4.081
            },
            {
                "subtitle": "college servers you&#39;re using somewhat",
                "start": 379.36,
                "dur": 3.44
            },
            {
                "subtitle": "like you have borrowed some kind of",
                "start": 381.28,
                "dur": 3.199
            },
            {
                "subtitle": "machines from your friend from your",
                "start": 382.8,
                "dur": 2.959
            },
            {
                "subtitle": "company you&#39;re working on the machines",
                "start": 384.479,
                "dur": 3.28
            },
            {
                "subtitle": "in your company so anywhere you do this",
                "start": 385.759,
                "dur": 4.401
            },
            {
                "subtitle": "it&#39;s very important to you know see what",
                "start": 387.759,
                "dur": 4.241
            },
            {
                "subtitle": "resources you have how many GPUs you",
                "start": 390.16,
                "dur": 3.28
            },
            {
                "subtitle": "have at your hand and how many you are",
                "start": 392,
                "dur": 3.199
            },
            {
                "subtitle": "able to use what&#39;s the memory",
                "start": 393.44,
                "dur": 4
            },
            {
                "subtitle": "distribution you can do among the GPUs",
                "start": 395.199,
                "dur": 5.761
            },
            {
                "subtitle": "cool so let&#39;s see so import torch now",
                "start": 397.44,
                "dur": 6.039
            },
            {
                "subtitle": "we&#39;re going to have",
                "start": 400.96,
                "dur": 6.56
            },
            {
                "subtitle": "import GPU PU util um now we&#39;re going to",
                "start": 403.479,
                "dur": 8.361
            },
            {
                "subtitle": "have import OS which is our um operating",
                "start": 407.52,
                "dur": 6.959
            },
            {
                "subtitle": "system and now we&#39;re going to do GPU",
                "start": 411.84,
                "dur": 4.88
            },
            {
                "subtitle": "util dot show just remember this code",
                "start": 414.479,
                "dur": 3.681
            },
            {
                "subtitle": "this is very important because this will",
                "start": 416.72,
                "dur": 4.16
            },
            {
                "subtitle": "help you forever to understand how many",
                "start": 418.16,
                "dur": 4.4
            },
            {
                "subtitle": "resources you have at your hand how many",
                "start": 420.88,
                "dur": 4.159
            },
            {
                "subtitle": "you know what to use and whether someone",
                "start": 422.56,
                "dur": 4
            },
            {
                "subtitle": "else is using it or not whether there&#39;s",
                "start": 425.039,
                "dur": 3.361
            },
            {
                "subtitle": "an condition or not whether there&#39;s some",
                "start": 426.56,
                "dur": 3.759
            },
            {
                "subtitle": "issues some thread running demon thread",
                "start": 428.4,
                "dur": 3.6
            },
            {
                "subtitle": "which is the main thread that runs so",
                "start": 430.319,
                "dur": 3.44
            },
            {
                "subtitle": "there&#39;s many problems here so basically",
                "start": 432,
                "dur": 3.28
            },
            {
                "subtitle": "this gives you an idea of what you&#39;re",
                "start": 433.759,
                "dur": 2.88
            },
            {
                "subtitle": "dealing with, how much resources you",
                "start": 435.28,
                "dur": 3.759
            },
            {
                "subtitle": "have and what to what extent you can",
                "start": 436.639,
                "dur": 4.28
            },
            {
                "subtitle": "push your",
                "start": 439.039,
                "dur": 7
            },
            {
                "subtitle": "limits. So torch dot CUDA is",
                "start": 440.919,
                "dur": 8.12
            },
            {
                "subtitle": "available. Yeah. So torch.ca is",
                "start": 446.039,
                "dur": 5.56
            },
            {
                "subtitle": "available and we going to do print GPU",
                "start": 449.039,
                "dur": 3.481
            },
            {
                "subtitle": "is",
                "start": 451.599,
                "dur": 3.72
            },
            {
                "subtitle": "available and otherwise GPU not",
                "start": 452.52,
                "dur": 6.28
            },
            {
                "subtitle": "available. That&#39;s it. So this is the",
                "start": 455.319,
                "dur": 6.961
            },
            {
                "subtitle": "thing and then now you&#39;re going to do is",
                "start": 458.8,
                "dur": 7.6
            },
            {
                "subtitle": "OS dot environ",
                "start": 462.28,
                "dur": 5.96
            },
            {
                "subtitle": "uhment variable. So this basically sets",
                "start": 466.4,
                "dur": 3.519
            },
            {
                "subtitle": "the environment variable to what we",
                "start": 468.24,
                "dur": 2.84
            },
            {
                "subtitle": "want. So",
                "start": 469.919,
                "dur": 3.321
            },
            {
                "subtitle": "CUDA",
                "start": 471.08,
                "dur": 4.399
            },
            {
                "subtitle": "device",
                "start": 473.24,
                "dur": 6.239
            },
            {
                "subtitle": "order this is going to be",
                "start": 475.479,
                "dur": 8.201
            },
            {
                "subtitle": "PCI bus ID. I I&#39;m going to explain what",
                "start": 479.479,
                "dur": 5.56
            },
            {
                "subtitle": "this means. But let me just complete",
                "start": 483.68,
                "dur": 2.44
            },
            {
                "subtitle": "writing",
                "start": 485.039,
                "dur": 3.921
            },
            {
                "subtitle": "this visible devices is going to be zero",
                "start": 486.12,
                "dur": 4.04
            },
            {
                "subtitle": "because there&#39;s only one device that&#39;s",
                "start": 488.96,
                "dur": 2.88
            },
            {
                "subtitle": "visible. Okay. So let&#39;s come back to",
                "start": 490.16,
                "dur": 4.56
            },
            {
                "subtitle": "this um PCI bus ID actually. So this",
                "start": 491.84,
                "dur": 4.72
            },
            {
                "subtitle": "basically what does it mean? So",
                "start": 494.72,
                "dur": 4
            },
            {
                "subtitle": "basically our CUDA basically provides",
                "start": 496.56,
                "dur": 3.52
            },
            {
                "subtitle": "two main methods for ordering or",
                "start": 498.72,
                "dur": 3.199
            },
            {
                "subtitle": "numbering the GPUs. So there can many",
                "start": 500.08,
                "dur": 3.519
            },
            {
                "subtitle": "many number of GPUs, right? This is just",
                "start": 501.919,
                "dur": 3.28
            },
            {
                "subtitle": "a general example because it has only",
                "start": 503.599,
                "dur": 3.6
            },
            {
                "subtitle": "one. You could have skipped that. But if",
                "start": 505.199,
                "dur": 4.241
            },
            {
                "subtitle": "you have many GPUs, it basically PCI bus",
                "start": 507.199,
                "dur": 4.72
            },
            {
                "subtitle": "ID numbers GPUs based on their PCI bus",
                "start": 509.44,
                "dur": 4.64
            },
            {
                "subtitle": "locations. So basically every GPU is",
                "start": 511.919,
                "dur": 3.92
            },
            {
                "subtitle": "connected to our computer&#39;s motherboard",
                "start": 514.08,
                "dur": 4.079
            },
            {
                "subtitle": "through the PCI. Um you don&#39;t need to",
                "start": 515.839,
                "dur": 6.041
            },
            {
                "subtitle": "understand in this detail.",
                "start": 518.159,
                "dur": 3.721
            },
            {
                "subtitle": "So basically every GPU is connected to",
                "start": 534.08,
                "dur": 5.68
            },
            {
                "subtitle": "your mother&#39;s oh so basically every GPU",
                "start": 535.839,
                "dur": 5.041
            },
            {
                "subtitle": "is connected to your computer&#39;s",
                "start": 539.76,
                "dur": 3.199
            },
            {
                "subtitle": "motherboard through the PCI bus. So you",
                "start": 540.88,
                "dur": 3.44
            },
            {
                "subtitle": "don&#39;t need to understand this in so much",
                "start": 542.959,
                "dur": 3.041
            },
            {
                "subtitle": "detail but it&#39;s sort of like a",
                "start": 544.32,
                "dur": 3.199
            },
            {
                "subtitle": "connection that you have and it&#39;s used",
                "start": 546,
                "dur": 3.68
            },
            {
                "subtitle": "for ordering or numbering the GPUs. So",
                "start": 547.519,
                "dur": 3.601
            },
            {
                "subtitle": "this can help you when you have multiple",
                "start": 549.68,
                "dur": 4.159
            },
            {
                "subtitle": "GPUs but otherwise it&#39;s fine. Um that&#39;s",
                "start": 551.12,
                "dur": 5.279
            },
            {
                "subtitle": "it. So let&#39;s move on. So let&#39;s run this",
                "start": 553.839,
                "dur": 6.961
            },
            {
                "subtitle": "and let&#39;s move on. So you will see what",
                "start": 556.399,
                "dur": 6
            },
            {
                "subtitle": "the resources look like when you&#39;re",
                "start": 560.8,
                "dur": 3.68
            },
            {
                "subtitle": "doing this. See GPU is available and we",
                "start": 562.399,
                "dur": 4
            },
            {
                "subtitle": "have no usage of GPU right at the moment",
                "start": 564.48,
                "dur": 4.24
            },
            {
                "subtitle": "but GPU is available and we can use it.",
                "start": 566.399,
                "dur": 4.641
            },
            {
                "subtitle": "Great. Sounds good. So now let&#39;s move on",
                "start": 568.72,
                "dur": 4.72
            },
            {
                "subtitle": "to the next thing. So we have import",
                "start": 571.04,
                "dur": 4.96
            },
            {
                "subtitle": "torch. We have import transformers. Oh,",
                "start": 573.44,
                "dur": 4.28
            },
            {
                "subtitle": "we&#39;ve already imported torch but that&#39;s",
                "start": 576,
                "dur": 4.6
            },
            {
                "subtitle": "okay. It doesn&#39;t matter. Uh from",
                "start": 577.72,
                "dur": 5.88
            },
            {
                "subtitle": "transformers we&#39;re going to import auto",
                "start": 580.6,
                "dur": 5.64
            },
            {
                "subtitle": "tokenizer.",
                "start": 583.6,
                "dur": 4.359
            },
            {
                "subtitle": "and auto",
                "start": 586.24,
                "dur": 5.56
            },
            {
                "subtitle": "model for um okay coer",
                "start": 587.959,
                "dur": 8.401
            },
            {
                "subtitle": "lm and we going to import bits and byes",
                "start": 591.8,
                "dur": 7.159
            },
            {
                "subtitle": "config well I did not want to",
                "start": 596.36,
                "dur": 5.4
            },
            {
                "subtitle": "autocomplete everything but it doesn&#39;t",
                "start": 598.959,
                "dur": 5.641
            },
            {
                "subtitle": "matter we are going to import llama",
                "start": 601.76,
                "dur": 5.759
            },
            {
                "subtitle": "tokenizer and uh from hugging face next",
                "start": 604.6,
                "dur": 4.48
            },
            {
                "subtitle": "we are going to",
                "start": 607.519,
                "dur": 5.44
            },
            {
                "subtitle": "import from hugging face because we&#39;ll",
                "start": 609.08,
                "dur": 7.64
            },
            {
                "subtitle": "need the model from there and also yeah",
                "start": 612.959,
                "dur": 5.921
            },
            {
                "subtitle": "data set is ours but model we&#39;ll need",
                "start": 616.72,
                "dur": 7.04
            },
            {
                "subtitle": "from there from hugging face hub import",
                "start": 618.88,
                "dur": 6.72
            },
            {
                "subtitle": "notebook login you will need to login to",
                "start": 623.76,
                "dur": 3.28
            },
            {
                "subtitle": "the notebook because llama model that",
                "start": 625.6,
                "dur": 3.08
            },
            {
                "subtitle": "we&#39;re going to use is",
                "start": 627.04,
                "dur": 4.32
            },
            {
                "subtitle": "actually it needs to be given access to",
                "start": 628.68,
                "dur": 4.12
            },
            {
                "subtitle": "so basically you will get access don&#39;t",
                "start": 631.36,
                "dur": 3.12
            },
            {
                "subtitle": "worry about it but there are a few steps",
                "start": 632.8,
                "dur": 2.96
            },
            {
                "subtitle": "I&#39;ll show you all the steps so don&#39;t",
                "start": 634.48,
                "dur": 3.2
            },
            {
                "subtitle": "worry about it but it will take some",
                "start": 635.76,
                "dur": 5.84
            },
            {
                "subtitle": "time to get the access so yeah for me it",
                "start": 637.68,
                "dur": 6.159
            },
            {
                "subtitle": "took around 15 minutes but for you might",
                "start": 641.6,
                "dur": 5.679
            },
            {
                "subtitle": "be longer but yeah it uh you will get it",
                "start": 643.839,
                "dur": 6.801
            },
            {
                "subtitle": "within definitely few hours. So I&#39;ll",
                "start": 647.279,
                "dur": 5.921
            },
            {
                "subtitle": "come back to it. Wait. Um now next thing",
                "start": 650.64,
                "dur": 6.24
            },
            {
                "subtitle": "is from PFT",
                "start": 653.2,
                "dur": 3.68
            },
            {
                "subtitle": "import",
                "start": 658.68,
                "dur": 6.8
            },
            {
                "subtitle": "prepare model for K bit",
                "start": 660.519,
                "dur": 8.041
            },
            {
                "subtitle": "training for 4bit quization. I I&#39;ll come",
                "start": 665.48,
                "dur": 5.32
            },
            {
                "subtitle": "in detail but this is what we need. And",
                "start": 668.56,
                "dur": 5.92
            },
            {
                "subtitle": "then we are going to need Lora config.",
                "start": 670.8,
                "dur": 5.96
            },
            {
                "subtitle": "We&#39;re going to use Lora for the 4bit",
                "start": 674.48,
                "dur": 4.359
            },
            {
                "subtitle": "quantization. And",
                "start": 676.76,
                "dur": 7.88
            },
            {
                "subtitle": "then get PFT model. And that&#39;s it. And",
                "start": 678.839,
                "dur": 8.12
            },
            {
                "subtitle": "then",
                "start": 684.64,
                "dur": 2.319
            },
            {
                "subtitle": "from that&#39;s okay. We already have",
                "start": 687.24,
                "dur": 4.279
            },
            {
                "subtitle": "everything. Yeah. So let&#39;s move on. So",
                "start": 689.44,
                "dur": 4.959
            },
            {
                "subtitle": "if we are using collab GPU, so which we",
                "start": 691.519,
                "dur": 6.401
            },
            {
                "subtitle": "are. So just give a short notice this",
                "start": 694.399,
                "dur": 5.841
            },
            {
                "subtitle": "one to say that if you&#39;re using your",
                "start": 697.92,
                "dur": 4.56
            },
            {
                "subtitle": "computer&#39;s GPU or something else then",
                "start": 700.24,
                "dur": 7.76
            },
            {
                "subtitle": "this will not be um activated. So yeah,",
                "start": 702.48,
                "dur": 6.88
            },
            {
                "subtitle": "so this is just to make your lives",
                "start": 708,
                "dur": 3.079
            },
            {
                "subtitle": "easier",
                "start": 709.36,
                "dur": 4
            },
            {
                "subtitle": "and import output. We&#39;re going to be",
                "start": 711.079,
                "dur": 3.88
            },
            {
                "subtitle": "able to see the output properly. So that",
                "start": 713.36,
                "dur": 3.52
            },
            {
                "subtitle": "is why we&#39;re doing this. Um this",
                "start": 714.959,
                "dur": 4.12
            },
            {
                "subtitle": "actually enhances the output format a",
                "start": 716.88,
                "dur": 5.72
            },
            {
                "subtitle": "lot. output dot enable custom widget",
                "start": 719.079,
                "dur": 7.481
            },
            {
                "subtitle": "manager. Great.",
                "start": 722.6,
                "dur": 7.16
            },
            {
                "subtitle": "Sounds good. Now we&#39;re going to do is",
                "start": 726.56,
                "dur": 5.48
            },
            {
                "subtitle": "Now we&#39;re going to do the notebook",
                "start": 729.76,
                "dur": 5.24
            },
            {
                "subtitle": "login. Collab",
                "start": 732.04,
                "dur": 7.64
            },
            {
                "subtitle": "GPU in OS dot environment. We going to",
                "start": 735,
                "dur": 7.44
            },
            {
                "subtitle": "do not equal",
                "start": 739.68,
                "dur": 8.04
            },
            {
                "subtitle": "to again past um hugging",
                "start": 742.44,
                "dur": 7.68
            },
            {
                "subtitle": "base cli",
                "start": 747.72,
                "dur": 7.6
            },
            {
                "subtitle": "login and then else we&#39;re going to do",
                "start": 750.12,
                "dur": 7.12
            },
            {
                "subtitle": "notebook",
                "start": 755.32,
                "dur": 3.959
            },
            {
                "subtitle": "login. Let&#39;s see. We&#39;re going to do the",
                "start": 757.24,
                "dur": 4.44
            },
            {
                "subtitle": "hugging page login. Exactly. So this is",
                "start": 759.279,
                "dur": 6
            },
            {
                "subtitle": "the token. Go in here. Yeah. This is",
                "start": 761.68,
                "dur": 5.52
            },
            {
                "subtitle": "what your token. You can do yes, no.",
                "start": 765.279,
                "dur": 4.56
            },
            {
                "subtitle": "It&#39;s up to you. And now login is valid.",
                "start": 767.2,
                "dur": 4.48
            },
            {
                "subtitle": "It has been shared. And now the current",
                "start": 769.839,
                "dur": 3.761
            },
            {
                "subtitle": "active login is login. Great. Now you",
                "start": 771.68,
                "dur": 3.44
            },
            {
                "subtitle": "have logged in. So it now knows which",
                "start": 773.6,
                "dur": 4.64
            },
            {
                "subtitle": "models you have access to. Yeah. Now you",
                "start": 775.12,
                "dur": 4.959
            },
            {
                "subtitle": "can see right here. So these are the",
                "start": 778.24,
                "dur": 3.68
            },
            {
                "subtitle": "gated repositories that I have access to",
                "start": 780.079,
                "dur": 3.601
            },
            {
                "subtitle": "and I have asked for the access to these",
                "start": 781.92,
                "dur": 4.08
            },
            {
                "subtitle": "models. So let me show you. So this is",
                "start": 783.68,
                "dur": 3.44
            },
            {
                "subtitle": "the model that we are going to use",
                "start": 786,
                "dur": 4.88
            },
            {
                "subtitle": "today. Metal lama 27B chat. Right? So",
                "start": 787.12,
                "dur": 5.92
            },
            {
                "subtitle": "for this model you need to get access.",
                "start": 790.88,
                "dur": 4
            },
            {
                "subtitle": "So if you see here you have been granted",
                "start": 793.04,
                "dur": 3.68
            },
            {
                "subtitle": "access to this model. So otherwise what",
                "start": 794.88,
                "dur": 4.8
            },
            {
                "subtitle": "you will see is you will see a list of",
                "start": 796.72,
                "dur": 6.32
            },
            {
                "subtitle": "something. Let me see if I have if I",
                "start": 799.68,
                "dur": 5.279
            },
            {
                "subtitle": "have access to this. If I don&#39;t have",
                "start": 803.04,
                "dur": 4.64
            },
            {
                "subtitle": "access to this yeah see you need to",
                "start": 804.959,
                "dur": 4.401
            },
            {
                "subtitle": "share contact information with meta to",
                "start": 807.68,
                "dur": 3.04
            },
            {
                "subtitle": "access the model. So this is what you",
                "start": 809.36,
                "dur": 3.2
            },
            {
                "subtitle": "will see when you are doing this chat",
                "start": 810.72,
                "dur": 3.359
            },
            {
                "subtitle": "thing which I have already been granted",
                "start": 812.56,
                "dur": 3.68
            },
            {
                "subtitle": "access because I&#39;ve run the code before.",
                "start": 814.079,
                "dur": 4.721
            },
            {
                "subtitle": "So this one you&#39;ll have to review and",
                "start": 816.24,
                "dur": 3.76
            },
            {
                "subtitle": "then you have to give your first name,",
                "start": 818.8,
                "dur": 2.839
            },
            {
                "subtitle": "last name, date of birth, country,",
                "start": 820,
                "dur": 3.92
            },
            {
                "subtitle": "affiliation. It can be your college. It",
                "start": 821.639,
                "dur": 3.961
            },
            {
                "subtitle": "can be your school. It can be the",
                "start": 823.92,
                "dur": 3.28
            },
            {
                "subtitle": "company you&#39;re working in. It can be",
                "start": 825.6,
                "dur": 3.84
            },
            {
                "subtitle": "anything. It doesn&#39;t really matter. And",
                "start": 827.2,
                "dur": 4
            },
            {
                "subtitle": "then just click submit and submit.",
                "start": 829.44,
                "dur": 3.68
            },
            {
                "subtitle": "That&#39;s it. You will get it within 15",
                "start": 831.2,
                "dur": 4.079
            },
            {
                "subtitle": "minutes. You will see it appearing here.",
                "start": 833.12,
                "dur": 3.839
            },
            {
                "subtitle": "These are the models for which you have",
                "start": 835.279,
                "dur": 4.56
            },
            {
                "subtitle": "added asked access for. So, as you can",
                "start": 836.959,
                "dur": 4.641
            },
            {
                "subtitle": "see, this one I haven&#39;t yet got access.",
                "start": 839.839,
                "dur": 4.081
            },
            {
                "subtitle": "I asked on October 4th. But yeah, that&#39;s",
                "start": 841.6,
                "dur": 3.84
            },
            {
                "subtitle": "okay. I don&#39;t really need it anymore.",
                "start": 843.92,
                "dur": 3.039
            },
            {
                "subtitle": "But this one I needed. So, I got it",
                "start": 845.44,
                "dur": 3.12
            },
            {
                "subtitle": "really fast. So, it will get accepted",
                "start": 846.959,
                "dur": 3.521
            },
            {
                "subtitle": "easily, I&#39;m telling you. So, just don&#39;t",
                "start": 848.56,
                "dur": 3.88
            },
            {
                "subtitle": "worry about it. and we can use",
                "start": 850.48,
                "dur": 5.28
            },
            {
                "subtitle": "it. Sounds good. Now just you need to",
                "start": 852.44,
                "dur": 5.24
            },
            {
                "subtitle": "wait for a few minutes. Just wait here.",
                "start": 855.76,
                "dur": 5.079
            },
            {
                "subtitle": "That&#39;s it. Don&#39;t worry about",
                "start": 857.68,
                "dur": 5.92
            },
            {
                "subtitle": "it. Okay. So the next thing that comes",
                "start": 860.839,
                "dur": 5.161
            },
            {
                "subtitle": "is base model ID. So now we going to",
                "start": 863.6,
                "dur": 4.64
            },
            {
                "subtitle": "provide the base model ID. So because on",
                "start": 866,
                "dur": 4.44
            },
            {
                "subtitle": "which we are going to do the",
                "start": 868.24,
                "dur": 6.36
            },
            {
                "subtitle": "finetuning. This is going to be metal",
                "start": 870.44,
                "dur": 7.12
            },
            {
                "subtitle": "lama llama",
                "start": 874.6,
                "dur": 7.919
            },
            {
                "subtitle": "27B chat HF. this one which I just now",
                "start": 877.56,
                "dur": 8.04
            },
            {
                "subtitle": "said for this you will need access so",
                "start": 882.519,
                "dur": 4.921
            },
            {
                "subtitle": "just get the access don&#39;t worry about it",
                "start": 885.6,
                "dur": 4.32
            },
            {
                "subtitle": "bnb config this is going to be the",
                "start": 887.44,
                "dur": 3.759
            },
            {
                "subtitle": "config in which we&#39;re going to load the",
                "start": 889.92,
                "dur": 2.719
            },
            {
                "subtitle": "model because we&#39;re going to do it in",
                "start": 891.199,
                "dur": 5.721
            },
            {
                "subtitle": "4bit quantization so that&#39;s it bits and",
                "start": 892.639,
                "dur": 5.801
            },
            {
                "subtitle": "byes",
                "start": 896.92,
                "dur": 4.84
            },
            {
                "subtitle": "config and then it&#39;s going to be it&#39;s",
                "start": 898.44,
                "dur": 5.639
            },
            {
                "subtitle": "going to be simple from here yeah let&#39;s",
                "start": 901.76,
                "dur": 4.519
            },
            {
                "subtitle": "see load in 4",
                "start": 904.079,
                "dur": 5.601
            },
            {
                "subtitle": "bit load in for bit is true bnb 4bit",
                "start": 906.279,
                "dur": 5.401
            },
            {
                "subtitle": "whose content double quant is true and",
                "start": 909.68,
                "dur": 4.48
            },
            {
                "subtitle": "then we have four bit quant type NF4 and",
                "start": 911.68,
                "dur": 4.64
            },
            {
                "subtitle": "this is the compute D type which is B",
                "start": 914.16,
                "dur": 4.239
            },
            {
                "subtitle": "floor 16 what does it mean let me come",
                "start": 916.32,
                "dur": 4.319
            },
            {
                "subtitle": "in detail it&#39;s okay so four bit",
                "start": 918.399,
                "dur": 3.521
            },
            {
                "subtitle": "quantization you understand we&#39;re going",
                "start": 920.639,
                "dur": 3.361
            },
            {
                "subtitle": "to load in 4 bit that is why so this",
                "start": 921.92,
                "dur": 3.76
            },
            {
                "subtitle": "basically means that we are going to do",
                "start": 924,
                "dur": 3.44
            },
            {
                "subtitle": "double quantization because it&#39;s",
                "start": 925.68,
                "dur": 4.32
            },
            {
                "subtitle": "basically saves memory so much so we",
                "start": 927.44,
                "dur": 4.32
            },
            {
                "subtitle": "were loading in 4 bit and then we are",
                "start": 930,
                "dur": 3.68
            },
            {
                "subtitle": "quantizing it again on top of it so that",
                "start": 931.76,
                "dur": 4.24
            },
            {
                "subtitle": "is what is called double quantization",
                "start": 933.68,
                "dur": 4.959
            },
            {
                "subtitle": "and this is because it will make it so",
                "start": 936,
                "dur": 4.56
            },
            {
                "subtitle": "much smaller without much loss in",
                "start": 938.639,
                "dur": 3.281
            },
            {
                "subtitle": "accuracy. We are not really looking for",
                "start": 940.56,
                "dur": 3.519
            },
            {
                "subtitle": "accuracy right now. But if you don&#39;t",
                "start": 941.92,
                "dur": 3.68
            },
            {
                "subtitle": "want to do this, you can you can do it",
                "start": 944.079,
                "dur": 2.961
            },
            {
                "subtitle": "too. You can just remove this line,",
                "start": 945.6,
                "dur": 4.239
            },
            {
                "subtitle": "comment out this line. It&#39;s up to you.",
                "start": 947.04,
                "dur": 4.56
            },
            {
                "subtitle": "And this is the quant type. So this is",
                "start": 949.839,
                "dur": 4
            },
            {
                "subtitle": "just a like a quant type that we follow",
                "start": 951.6,
                "dur": 5.2
            },
            {
                "subtitle": "generally. So it basically um uses this",
                "start": 953.839,
                "dur": 5.68
            },
            {
                "subtitle": "normal float 4 and NF4 is optimized for",
                "start": 956.8,
                "dur": 4.88
            },
            {
                "subtitle": "normal distributions in neural networks.",
                "start": 959.519,
                "dur": 4.161
            },
            {
                "subtitle": "So it basically preserves model accuracy",
                "start": 961.68,
                "dur": 3.44
            },
            {
                "subtitle": "as well. So we don&#39;t want to because",
                "start": 963.68,
                "dur": 2.88
            },
            {
                "subtitle": "we&#39;re doing double quantization. This",
                "start": 965.12,
                "dur": 3.68
            },
            {
                "subtitle": "will lead to a big hit in the accuracy.",
                "start": 966.56,
                "dur": 4.8
            },
            {
                "subtitle": "But we are preserving that using NF4. So",
                "start": 968.8,
                "dur": 5.2
            },
            {
                "subtitle": "that&#39;s it. And this is just B floor 16.",
                "start": 971.36,
                "dur": 4.8
            },
            {
                "subtitle": "This is the generally the data type that",
                "start": 974,
                "dur": 4.32
            },
            {
                "subtitle": "we generally use for fine-tuning and for",
                "start": 976.16,
                "dur": 4.32
            },
            {
                "subtitle": "loading. So B floor 16, remember this B",
                "start": 978.32,
                "dur": 4.16
            },
            {
                "subtitle": "floor 16. We&#39;re going to use it forever.",
                "start": 980.48,
                "dur": 4.08
            },
            {
                "subtitle": "Don&#39;t use floor 16 normal. And don&#39;t use",
                "start": 982.48,
                "dur": 4.24
            },
            {
                "subtitle": "FP32. Don&#39;t use anything else, but use B",
                "start": 984.56,
                "dur": 4.12
            },
            {
                "subtitle": "floor",
                "start": 986.72,
                "dur": 5.28
            },
            {
                "subtitle": "16. Sounds good. Now let&#39;s load the",
                "start": 988.68,
                "dur": 9
            },
            {
                "subtitle": "model. Auto model for causal LM. Um,",
                "start": 992,
                "dur": 9.279
            },
            {
                "subtitle": "sorry. dot from",
                "start": 997.68,
                "dur": 3.599
            },
            {
                "subtitle": "pre-trained. Okay. Auto model from",
                "start": 1001.88,
                "dur": 4.6
            },
            {
                "subtitle": "causal LM pro dot from pre-trained. We",
                "start": 1004.16,
                "dur": 4.32
            },
            {
                "subtitle": "are going to do the base model ID that",
                "start": 1006.48,
                "dur": 4.159
            },
            {
                "subtitle": "we have already determined uh written",
                "start": 1008.48,
                "dur": 4.88
            },
            {
                "subtitle": "here. And then",
                "start": 1010.639,
                "dur": 2.721
            },
            {
                "subtitle": "quantization config would be equal to",
                "start": 1014.839,
                "dur": 4.8
            },
            {
                "subtitle": "BNB",
                "start": 1018.24,
                "dur": 4.36
            },
            {
                "subtitle": "config. That&#39;s it. I don&#39;t need anything",
                "start": 1019.639,
                "dur": 7.04
            },
            {
                "subtitle": "else. It&#39;s more than enough. BNB",
                "start": 1022.6,
                "dur": 7.4
            },
            {
                "subtitle": "config. That&#39;s it. So quantation config",
                "start": 1026.679,
                "dur": 5.36
            },
            {
                "subtitle": "is going to be PNB",
                "start": 1030,
                "dur": 5.679
            },
            {
                "subtitle": "config. Let&#39;s load the",
                "start": 1032.039,
                "dur": 3.64
            },
            {
                "subtitle": "model. Okay. I think I missed something.",
                "start": 1037.319,
                "dur": 6.12
            },
            {
                "subtitle": "Let me see. Llama 27B. Let me just copy",
                "start": 1040.319,
                "dur": 6
            },
            {
                "subtitle": "this. And it&#39;s a chat. It should have",
                "start": 1043.439,
                "dur": 5.921
            },
            {
                "subtitle": "been chat. Yeah. Sorry.",
                "start": 1046.319,
                "dur": 3.041
            },
            {
                "subtitle": "Sorry.",
                "start": 1052.52,
                "dur": 3
            },
            {
                "subtitle": "Um, wait a second. Let me",
                "start": 1056.2,
                "dur": 4.44
            },
            {
                "subtitle": "check. Oh, right. Right. HF. We need the",
                "start": 1061.559,
                "dur": 5.48
            },
            {
                "subtitle": "hugging face type. HF is something we&#39;ll",
                "start": 1064.32,
                "dur": 4.4
            },
            {
                "subtitle": "need otherwise it will not work. This is",
                "start": 1067.039,
                "dur": 2.801
            },
            {
                "subtitle": "something. This is a bit different",
                "start": 1068.72,
                "dur": 3.12
            },
            {
                "subtitle": "model. There are a lot of variations.",
                "start": 1069.84,
                "dur": 3.52
            },
            {
                "subtitle": "Yeah, see now it&#39;s working. So, this",
                "start": 1071.84,
                "dur": 2.719
            },
            {
                "subtitle": "model we are going to load and we&#39;re",
                "start": 1073.36,
                "dur": 2.64
            },
            {
                "subtitle": "going to fine-tune this model on our",
                "start": 1074.559,
                "dur": 3.281
            },
            {
                "subtitle": "personal data set which is a bunch of",
                "start": 1076,
                "dur": 5.12
            },
            {
                "subtitle": "text files. Cool. As long as this is",
                "start": 1077.84,
                "dur": 5.12
            },
            {
                "subtitle": "waiting. So let&#39;s start writing the next",
                "start": 1081.12,
                "dur": 3.559
            },
            {
                "subtitle": "part of the code. We don&#39;t want to waste",
                "start": 1082.96,
                "dur": 5.28
            },
            {
                "subtitle": "time. Get clone. Let&#39;s see. I&#39;ll clone",
                "start": 1084.679,
                "dur": 5.321
            },
            {
                "subtitle": "the repository here which has a bunch of",
                "start": 1088.24,
                "dur": 5.76
            },
            {
                "subtitle": "text files. So",
                "start": 1090,
                "dur": 4
            },
            {
                "subtitle": "yeah,",
                "start": 1094.2,
                "dur": 3
            },
            {
                "subtitle": "github.com club and then find I could",
                "start": 1099.799,
                "dur": 6.521
            },
            {
                "subtitle": "find the easiest example bunch of text",
                "start": 1104.08,
                "dur": 4.24
            },
            {
                "subtitle": "documents online. So that is why I use",
                "start": 1106.32,
                "dur": 3.68
            },
            {
                "subtitle": "this data set. You can use anything. I&#39;m",
                "start": 1108.32,
                "dur": 3.52
            },
            {
                "subtitle": "telling you you can use anything. This",
                "start": 1110,
                "dur": 4.799
            },
            {
                "subtitle": "is the most simple thing ever possible.",
                "start": 1111.84,
                "dur": 4.719
            },
            {
                "subtitle": "Let&#39;s see. Let me show you what the Git",
                "start": 1114.799,
                "dur": 4.201
            },
            {
                "subtitle": "repository looks",
                "start": 1116.559,
                "dur": 4.881
            },
            {
                "subtitle": "like. We have data. See, there&#39;s just",
                "start": 1119,
                "dur": 4.36
            },
            {
                "subtitle": "like a text document, normal text",
                "start": 1121.44,
                "dur": 3.68
            },
            {
                "subtitle": "document talking about the Hawaiian",
                "start": 1123.36,
                "dur": 3.92
            },
            {
                "subtitle": "wildfire. The first text document, the",
                "start": 1125.12,
                "dur": 3.679
            },
            {
                "subtitle": "second text document, nothing. Nothing",
                "start": 1127.28,
                "dur": 3.48
            },
            {
                "subtitle": "like this. Just a bunch of text",
                "start": 1128.799,
                "dur": 4.401
            },
            {
                "subtitle": "documents. Simple written. No",
                "start": 1130.76,
                "dur": 4.919
            },
            {
                "subtitle": "formatting, no bullet points, nothing.",
                "start": 1133.2,
                "dur": 4.88
            },
            {
                "subtitle": "Literally no tokens, anything. Just get",
                "start": 1135.679,
                "dur": 4.481
            },
            {
                "subtitle": "whatever you want. It can be CSV file.",
                "start": 1138.08,
                "dur": 3.44
            },
            {
                "subtitle": "I&#39;m telling you. It can be CSV file,",
                "start": 1140.16,
                "dur": 3.84
            },
            {
                "subtitle": "word document, PDF documents. It can be",
                "start": 1141.52,
                "dur": 4.88
            },
            {
                "subtitle": "word documents. It can be Excel, Excel",
                "start": 1144,
                "dur": 4.16
            },
            {
                "subtitle": "sheets, presentations, anything you",
                "start": 1146.4,
                "dur": 5.04
            },
            {
                "subtitle": "want. All is fine. So this is what we",
                "start": 1148.16,
                "dur": 4.96
            },
            {
                "subtitle": "going to use today. This is going on. So",
                "start": 1151.44,
                "dur": 2.96
            },
            {
                "subtitle": "this is the get clone that we are going",
                "start": 1153.12,
                "dur": 3.52
            },
            {
                "subtitle": "to do. And we&#39;re going to put it here.",
                "start": 1154.4,
                "dur": 3.68
            },
            {
                "subtitle": "We&#39;re going to bring the data set out",
                "start": 1156.64,
                "dur": 3.12
            },
            {
                "subtitle": "because we don&#39;t want to put it inside",
                "start": 1158.08,
                "dur": 3.599
            },
            {
                "subtitle": "this data repository because we&#39;re going",
                "start": 1159.76,
                "dur": 5.039
            },
            {
                "subtitle": "to use the like the home repository. So",
                "start": 1161.679,
                "dur": 7.641
            },
            {
                "subtitle": "let&#39;s do that as the next step.",
                "start": 1164.799,
                "dur": 4.521
            },
            {
                "subtitle": "Yeah, we can do that when this cloning",
                "start": 1171.36,
                "dur": 3.92
            },
            {
                "subtitle": "is done. But for now, let&#39;s write the",
                "start": 1173.12,
                "dur": 4.799
            },
            {
                "subtitle": "next part of the",
                "start": 1175.28,
                "dur": 2.639
            },
            {
                "subtitle": "code. So, train data set is going to",
                "start": 1178.44,
                "dur": 6
            },
            {
                "subtitle": "be load data",
                "start": 1181.88,
                "dur": 5.48
            },
            {
                "subtitle": "set. Yeah, load data set. And then this",
                "start": 1184.44,
                "dur": 4.08
            },
            {
                "subtitle": "is going to be",
                "start": 1187.36,
                "dur": 4.6
            },
            {
                "subtitle": "text. We&#39;re going to load text data",
                "start": 1188.52,
                "dur": 9.039
            },
            {
                "subtitle": "set. And data files is going to be",
                "start": 1191.96,
                "dur": 5.599
            },
            {
                "subtitle": "train and this is going to be a list. So",
                "start": 1200.52,
                "dur": 8.68
            },
            {
                "subtitle": "list is going to be what? Content",
                "start": 1205.12,
                "dur": 4.08
            },
            {
                "subtitle": "slash well it&#39;s correct but it&#39;s not too",
                "start": 1209.24,
                "dur": 6.24
            },
            {
                "subtitle": "much correct.",
                "start": 1212.64,
                "dur": 5.84
            },
            {
                "subtitle": "Finetuning",
                "start": 1215.48,
                "dur": 3
            },
            {
                "subtitle": "llm/data/i",
                "start": 1221.32,
                "dur": 5.12
            },
            {
                "subtitle": "Hawaii wf1.tx",
                "start": 1222.84,
                "dur": 6.52
            },
            {
                "subtitle": "txt. This is the first one and like this",
                "start": 1226.44,
                "dur": 6.68
            },
            {
                "subtitle": "we are having a lot of data. So how many",
                "start": 1229.36,
                "dur": 6.4
            },
            {
                "subtitle": "data do we have? We have 10. We have 10.",
                "start": 1233.12,
                "dur": 4.28
            },
            {
                "subtitle": "Yeah,",
                "start": 1235.76,
                "dur": 6.96
            },
            {
                "subtitle": "10. So let me just copy this out 10",
                "start": 1237.4,
                "dur": 5.32
            },
            {
                "subtitle": "times. Or we can just use two for now. I",
                "start": 1242.919,
                "dur": 6.601
            },
            {
                "subtitle": "I mean it&#39;s just for example purposes.",
                "start": 1247.12,
                "dur": 4.08
            },
            {
                "subtitle": "So you can put in any number of data you",
                "start": 1249.52,
                "dur": 4.32
            },
            {
                "subtitle": "want and that&#39;s up to you. It&#39;s fine.",
                "start": 1251.2,
                "dur": 5.04
            },
            {
                "subtitle": "Okay. Let&#39;s let&#39;s keep two only. Um I",
                "start": 1253.84,
                "dur": 3.44
            },
            {
                "subtitle": "don&#39;t want to make it too much",
                "start": 1256.24,
                "dur": 2.799
            },
            {
                "subtitle": "complicated or take I don&#39;t want it to",
                "start": 1257.28,
                "dur": 4.8
            },
            {
                "subtitle": "take too much time because again this is",
                "start": 1259.039,
                "dur": 5.721
            },
            {
                "subtitle": "because of training purposes sample",
                "start": 1262.08,
                "dur": 5
            },
            {
                "subtitle": "purposes. Split",
                "start": 1264.76,
                "dur": 5.24
            },
            {
                "subtitle": "train. This sounds good. Now the train",
                "start": 1267.08,
                "dur": 5
            },
            {
                "subtitle": "data set is coming in. Now the next",
                "start": 1270,
                "dur": 4.559
            },
            {
                "subtitle": "thing that comes in is yeah cloning is",
                "start": 1272.08,
                "dur": 5.56
            },
            {
                "subtitle": "done. The model loading is done.",
                "start": 1274.559,
                "dur": 5.841
            },
            {
                "subtitle": "Now yeah I think because this has not",
                "start": 1277.64,
                "dur": 7.24
            },
            {
                "subtitle": "been brought out. Let me see.",
                "start": 1280.4,
                "dur": 8.04
            },
            {
                "subtitle": "Yeah, we have here and we have in",
                "start": 1284.88,
                "dur": 7.279
            },
            {
                "subtitle": "data. Now we are going",
                "start": 1288.44,
                "dur": 3.719
            },
            {
                "subtitle": "to bring out bring it out outside. Are",
                "start": 1294.84,
                "dur": 7.76
            },
            {
                "subtitle": "we are we have been able to",
                "start": 1298.159,
                "dur": 4.441
            },
            {
                "subtitle": "um Okay, it doesn&#39;t really matter. It&#39;s",
                "start": 1307,
                "dur": 5.679
            },
            {
                "subtitle": "inside this, right? So, it&#39;s already in",
                "start": 1309.44,
                "dur": 7.68
            },
            {
                "subtitle": "the I think I lost",
                "start": 1312.679,
                "dur": 4.441
            },
            {
                "subtitle": "something. Whatever. I don&#39;t want to",
                "start": 1318.6,
                "dur": 3.959
            },
            {
                "subtitle": "waste too much time here. So, let&#39;s just",
                "start": 1320.48,
                "dur": 4.88
            },
            {
                "subtitle": "change it to three. Let&#39;s just change it",
                "start": 1322.559,
                "dur": 4.801
            },
            {
                "subtitle": "to three and let&#39;s just change it to two",
                "start": 1325.36,
                "dur": 4.48
            },
            {
                "subtitle": "and three. It is already there. So,",
                "start": 1327.36,
                "dur": 4.12
            },
            {
                "subtitle": "what&#39;s the",
                "start": 1329.84,
                "dur": 6.28
            },
            {
                "subtitle": "issue? Train data files equal to",
                "start": 1331.48,
                "dur": 7.16
            },
            {
                "subtitle": "train. Ah, got it. So this is going to",
                "start": 1336.12,
                "dur": 4.4
            },
            {
                "subtitle": "be a",
                "start": 1338.64,
                "dur": 4.399
            },
            {
                "subtitle": "dictionary. Cool. So this is going to be",
                "start": 1340.52,
                "dur": 4.68
            },
            {
                "subtitle": "a",
                "start": 1343.039,
                "dur": 2.161
            },
            {
                "subtitle": "dictionary,",
                "start": 1346.28,
                "dur": 5.16
            },
            {
                "subtitle": "right?",
                "start": 1348.44,
                "dur": 3
            },
            {
                "subtitle": "Um",
                "start": 1356.44,
                "dur": 5.64
            },
            {
                "subtitle": "okay. I think this is a dictionary and",
                "start": 1359.159,
                "dur": 6.601
            },
            {
                "subtitle": "this is going to be I&#39;m not very wrong.",
                "start": 1362.08,
                "dur": 5.839
            },
            {
                "subtitle": "And how I",
                "start": 1365.76,
                "dur": 3.64
            },
            {
                "subtitle": "A Y",
                "start": 1367.919,
                "dur": 5.361
            },
            {
                "subtitle": "WF3 we have that. So why is it saying",
                "start": 1369.4,
                "dur": 7.759
            },
            {
                "subtitle": "that? Is it a spelling mistake?",
                "start": 1373.28,
                "dur": 6.879
            },
            {
                "subtitle": "WF3",
                "start": 1377.159,
                "dur": 3
            },
            {
                "subtitle": "content/finetuning finetuning",
                "start": 1382.36,
                "dur": 4.199
            },
            {
                "subtitle": "llm/data/hawaii wf3. Okay, I am not able",
                "start": 1389.32,
                "dur": 7.92
            },
            {
                "subtitle": "to get why this is showing an",
                "start": 1393.12,
                "dur": 10.52
            },
            {
                "subtitle": "error. YWF maybe I can make it four or",
                "start": 1397.24,
                "dur": 11.04
            },
            {
                "subtitle": "something. Unable to find fine tuning.",
                "start": 1403.64,
                "dur": 7.8
            },
            {
                "subtitle": "Ah, see this kind of errors make your",
                "start": 1408.28,
                "dur": 6.04
            },
            {
                "subtitle": "life so much",
                "start": 1411.44,
                "dur": 2.88
            },
            {
                "subtitle": "difficult. Great. Now it works. So we",
                "start": 1415.799,
                "dur": 4.76
            },
            {
                "subtitle": "have the train split. Now the next thing",
                "start": 1418.72,
                "dur": 5.48
            },
            {
                "subtitle": "we are going to do is train data",
                "start": 1420.559,
                "dur": 8.401
            },
            {
                "subtitle": "set text zero. Let&#39;s see an example. So",
                "start": 1424.2,
                "dur": 7.4
            },
            {
                "subtitle": "the in early morning of 9 coordinated",
                "start": 1428.96,
                "dur": 4.719
            },
            {
                "subtitle": "the transaction. Let me see which one",
                "start": 1431.6,
                "dur": 5.16
            },
            {
                "subtitle": "did I take finally. It&#39;s the fourth",
                "start": 1433.679,
                "dur": 7.161
            },
            {
                "subtitle": "one. See this is the text coordinate",
                "start": 1436.76,
                "dur": 7.399
            },
            {
                "subtitle": "transportation for v who. So this is how",
                "start": 1440.84,
                "dur": 5.719
            },
            {
                "subtitle": "this data set has been created. So this",
                "start": 1444.159,
                "dur": 3.76
            },
            {
                "subtitle": "text file has been broken down into",
                "start": 1446.559,
                "dur": 3.201
            },
            {
                "subtitle": "lines and each line constitutes a part",
                "start": 1447.919,
                "dur": 4.961
            },
            {
                "subtitle": "of the data set. Let me if you are not",
                "start": 1449.76,
                "dur": 6.48
            },
            {
                "subtitle": "still believing me let&#39;s see one see had",
                "start": 1452.88,
                "dur": 5.12
            },
            {
                "subtitle": "taken refuge the entire thing is there",
                "start": 1456.24,
                "dur": 4.72
            },
            {
                "subtitle": "in the second line second element of the",
                "start": 1458,
                "dur": 5.679
            },
            {
                "subtitle": "data set. So that&#39;s it. Now let&#39;s move",
                "start": 1460.96,
                "dur": 4.56
            },
            {
                "subtitle": "on to the next one. So now we&#39;re going",
                "start": 1463.679,
                "dur": 4.801
            },
            {
                "subtitle": "to do is tokenizer. So what is the step",
                "start": 1465.52,
                "dur": 4.48
            },
            {
                "subtitle": "basically? So basically after getting",
                "start": 1468.48,
                "dur": 3.36
            },
            {
                "subtitle": "the data set our next step is always to",
                "start": 1470,
                "dur": 4.08
            },
            {
                "subtitle": "get the tokenizer and to tokenize the",
                "start": 1471.84,
                "dur": 4.56
            },
            {
                "subtitle": "text document the data set. So that next",
                "start": 1474.08,
                "dur": 3.92
            },
            {
                "subtitle": "step is setting up the training",
                "start": 1476.4,
                "dur": 3.68
            },
            {
                "subtitle": "arguments and all and then choosing the",
                "start": 1478,
                "dur": 4.4
            },
            {
                "subtitle": "correct library to train and then start",
                "start": 1480.08,
                "dur": 3.839
            },
            {
                "subtitle": "the training and the next thing is",
                "start": 1482.4,
                "dur": 4.08
            },
            {
                "subtitle": "inference and then we are done. It&#39;s",
                "start": 1483.919,
                "dur": 6.401
            },
            {
                "subtitle": "very simple just just pay focus and then",
                "start": 1486.48,
                "dur": 5.92
            },
            {
                "subtitle": "literally my fine tuning is the next",
                "start": 1490.32,
                "dur": 5.359
            },
            {
                "subtitle": "startup business in the next few years.",
                "start": 1492.4,
                "dur": 5.2
            },
            {
                "subtitle": "So I mean if you are really good at",
                "start": 1495.679,
                "dur": 3.201
            },
            {
                "subtitle": "finetuning you can start your own",
                "start": 1497.6,
                "dur": 3.16
            },
            {
                "subtitle": "startup as",
                "start": 1498.88,
                "dur": 4.72
            },
            {
                "subtitle": "well. Let&#39;s see what what you do with",
                "start": 1500.76,
                "dur": 4.68
            },
            {
                "subtitle": "this information.",
                "start": 1503.6,
                "dur": 4.84
            },
            {
                "subtitle": "So from",
                "start": 1505.44,
                "dur": 6.719
            },
            {
                "subtitle": "pre-trained base model",
                "start": 1508.44,
                "dur": 3.719
            },
            {
                "subtitle": "ID and used past equal to",
                "start": 1513.96,
                "dur": 7.76
            },
            {
                "subtitle": "false and then",
                "start": 1518.2,
                "dur": 6.76
            },
            {
                "subtitle": "trust remote code equal to true because",
                "start": 1521.72,
                "dur": 5.48
            },
            {
                "subtitle": "you&#39;re using hugging face library. So",
                "start": 1524.96,
                "dur": 3.599
            },
            {
                "subtitle": "you need to tell the Google collab that",
                "start": 1527.2,
                "dur": 3.52
            },
            {
                "subtitle": "it can able to it&#39;s able to trust the",
                "start": 1528.559,
                "dur": 5.12
            },
            {
                "subtitle": "hugging face code and you are going to",
                "start": 1530.72,
                "dur": 4.959
            },
            {
                "subtitle": "add end of sequence token as true",
                "start": 1533.679,
                "dur": 3.641
            },
            {
                "subtitle": "because you will need the token",
                "start": 1535.679,
                "dur": 5.48
            },
            {
                "subtitle": "ending for why would we need the token",
                "start": 1537.32,
                "dur": 6.32
            },
            {
                "subtitle": "ending? Well",
                "start": 1541.159,
                "dur": 4.841
            },
            {
                "subtitle": "because because it helps to mark the end",
                "start": 1543.64,
                "dur": 4.36
            },
            {
                "subtitle": "of the text sequences and it generates",
                "start": 1546,
                "dur": 4.799
            },
            {
                "subtitle": "wellformed outputs. So use fast equal to",
                "start": 1548,
                "dur": 4.4
            },
            {
                "subtitle": "false. Why are we doing that? Because it",
                "start": 1550.799,
                "dur": 3.281
            },
            {
                "subtitle": "ensures consistent tokenization during",
                "start": 1552.4,
                "dur": 4.08
            },
            {
                "subtitle": "finetuning. Otherwise fast tokenizer is",
                "start": 1554.08,
                "dur": 5.28
            },
            {
                "subtitle": "the rust based fast tokenizer um but it",
                "start": 1556.48,
                "dur": 5.76
            },
            {
                "subtitle": "it is not very like efficient um or",
                "start": 1559.36,
                "dur": 5.199
            },
            {
                "subtitle": "accurate. So more reliable for llama",
                "start": 1562.24,
                "dur": 4.08
            },
            {
                "subtitle": "model is basically during finetuning to",
                "start": 1564.559,
                "dur": 4.401
            },
            {
                "subtitle": "disable the fast one slower but it",
                "start": 1566.32,
                "dur": 4.68
            },
            {
                "subtitle": "prevents potential tokenization",
                "start": 1568.96,
                "dur": 4.48
            },
            {
                "subtitle": "inconsistencies and the US token is very",
                "start": 1571,
                "dur": 4.52
            },
            {
                "subtitle": "important. So this will this will be",
                "start": 1573.44,
                "dur": 4.56
            },
            {
                "subtitle": "really good. Okay. So now this part is",
                "start": 1575.52,
                "dur": 4.639
            },
            {
                "subtitle": "very very very important. Pay close",
                "start": 1578,
                "dur": 5.159
            },
            {
                "subtitle": "attention.",
                "start": 1580.159,
                "dur": 3
            },
            {
                "subtitle": "Okay. Okay. So this thing is very",
                "start": 1585.32,
                "dur": 4.359
            },
            {
                "subtitle": "important what I&#39;m going to talk about",
                "start": 1587.6,
                "dur": 4.959
            },
            {
                "subtitle": "here. So this is basically specific to",
                "start": 1589.679,
                "dur": 4.721
            },
            {
                "subtitle": "llama models. So what do I mean by that?",
                "start": 1592.559,
                "dur": 4.801
            },
            {
                "subtitle": "So basically here if the pad token is",
                "start": 1594.4,
                "dur": 4.879
            },
            {
                "subtitle": "none. So llama models don&#39;t actually",
                "start": 1597.36,
                "dur": 3.52
            },
            {
                "subtitle": "have the pad token. So that is why it&#39;s",
                "start": 1599.279,
                "dur": 3.921
            },
            {
                "subtitle": "very important to provide the pad token",
                "start": 1600.88,
                "dur": 3.88
            },
            {
                "subtitle": "to the tokenizer",
                "start": 1603.2,
                "dur": 3.92
            },
            {
                "subtitle": "otherwise llama models won&#39;t work on",
                "start": 1604.76,
                "dur": 5
            },
            {
                "subtitle": "like uneven data sets. So it doesn&#39;t",
                "start": 1607.12,
                "dur": 4.88
            },
            {
                "subtitle": "have any pad token. So just use end of",
                "start": 1609.76,
                "dur": 4.159
            },
            {
                "subtitle": "sequence token as the patch token. So",
                "start": 1612,
                "dur": 4.72
            },
            {
                "subtitle": "what we&#39;re going to do is",
                "start": 1613.919,
                "dur": 2.801
            },
            {
                "subtitle": "tokenizer dot add special",
                "start": 1617.4,
                "dur": 4.68
            },
            {
                "subtitle": "token token",
                "start": 1625.559,
                "dur": 4.281
            },
            {
                "subtitle": "as pad token and then this pad token",
                "start": 1630.2,
                "dur": 7.44
            },
            {
                "subtitle": "would be tokenizer end of sequence",
                "start": 1634.24,
                "dur": 5.16
            },
            {
                "subtitle": "token",
                "start": 1637.64,
                "dur": 5.48
            },
            {
                "subtitle": "eos token. That&#39;s it. You&#39;re loading the",
                "start": 1639.4,
                "dur": 5.8
            },
            {
                "subtitle": "tokenizer and you are going to provide",
                "start": 1643.12,
                "dur": 4.2
            },
            {
                "subtitle": "the bad token as the end of sequence",
                "start": 1645.2,
                "dur": 5.359
            },
            {
                "subtitle": "token. Now we going to make the",
                "start": 1647.32,
                "dur": 4.839
            },
            {
                "subtitle": "tokenize. This is very important. This",
                "start": 1650.559,
                "dur": 3.12
            },
            {
                "subtitle": "is how we are going to pre-process the",
                "start": 1652.159,
                "dur": 4.721
            },
            {
                "subtitle": "data set which is our personal",
                "start": 1653.679,
                "dur": 3.201
            },
            {
                "subtitle": "data. I think there was some timeout",
                "start": 1657.24,
                "dur": 6.12
            },
            {
                "subtitle": "issue. Let me try it",
                "start": 1659.84,
                "dur": 3.52
            },
            {
                "subtitle": "again. Um okay. So these things now",
                "start": 1664.52,
                "dur": 8.56
            },
            {
                "subtitle": "they add special",
                "start": 1669.559,
                "dur": 7.401
            },
            {
                "subtitle": "token I am",
                "start": 1673.08,
                "dur": 3.88
            },
            {
                "subtitle": "sorry. Yeah done. So these things you",
                "start": 1677.799,
                "dur": 5.48
            },
            {
                "subtitle": "need to keep in mind because I mean you",
                "start": 1680.88,
                "dur": 4.399
            },
            {
                "subtitle": "have chat GPT you have claw sign but",
                "start": 1683.279,
                "dur": 3.441
            },
            {
                "subtitle": "still just keep in mind when you&#39;re",
                "start": 1685.279,
                "dur": 2.801
            },
            {
                "subtitle": "doing your interviews or talking about",
                "start": 1686.72,
                "dur": 3.76
            },
            {
                "subtitle": "an exams whatever. So this tokenized",
                "start": 1688.08,
                "dur": 4.16
            },
            {
                "subtitle": "train data set is this. So this is very",
                "start": 1690.48,
                "dur": 3.12
            },
            {
                "subtitle": "important. This is how we are going to",
                "start": 1692.24,
                "dur": 3.919
            },
            {
                "subtitle": "make the our our own data set which is a",
                "start": 1693.6,
                "dur": 4.559
            },
            {
                "subtitle": "bunch of text files random text files",
                "start": 1696.159,
                "dur": 4
            },
            {
                "subtitle": "into a format that can be trained that",
                "start": 1698.159,
                "dur": 3.88
            },
            {
                "subtitle": "can be fine- tuned on the",
                "start": 1700.159,
                "dur": 5.161
            },
            {
                "subtitle": "model. So",
                "start": 1702.039,
                "dur": 5.921
            },
            {
                "subtitle": "for phrase",
                "start": 1705.32,
                "dur": 9.719
            },
            {
                "subtitle": "in train data set it&#39;s going to be",
                "start": 1707.96,
                "dur": 7.079
            },
            {
                "subtitle": "tokenized train data set dot",
                "start": 1715.08,
                "dur": 8.88
            },
            {
                "subtitle": "append tokenizer phrase",
                "start": 1720.279,
                "dur": 7
            },
            {
                "subtitle": "text. Okay. So what do I mean by this?",
                "start": 1723.96,
                "dur": 5.8
            },
            {
                "subtitle": "So basically in the train data set we",
                "start": 1727.279,
                "dur": 4.081
            },
            {
                "subtitle": "have the different kinds of phrases.",
                "start": 1729.76,
                "dur": 3.2
            },
            {
                "subtitle": "Right. So this is one phrase in the",
                "start": 1731.36,
                "dur": 3.679
            },
            {
                "subtitle": "train data set. See this is a phrase",
                "start": 1732.96,
                "dur": 3.76
            },
            {
                "subtitle": "right? This is not a complete sentence",
                "start": 1735.039,
                "dur": 4.721
            },
            {
                "subtitle": "is a phrase and tokenized train data set",
                "start": 1736.72,
                "dur": 4.8
            },
            {
                "subtitle": "will append the tokenized version of",
                "start": 1739.76,
                "dur": 5.039
            },
            {
                "subtitle": "this text. So okay so let&#39;s see an",
                "start": 1741.52,
                "dur": 5.6
            },
            {
                "subtitle": "example how this looks like. So let&#39;s",
                "start": 1744.799,
                "dur": 5.681
            },
            {
                "subtitle": "see an example. Let&#39;s first run this and",
                "start": 1747.12,
                "dur": 4.919
            },
            {
                "subtitle": "then let&#39;s see",
                "start": 1750.48,
                "dur": 6.079
            },
            {
                "subtitle": "tokenized train data set one.",
                "start": 1752.039,
                "dur": 6.601
            },
            {
                "subtitle": "See, so this is a tokenized train data",
                "start": 1756.559,
                "dur": 3.761
            },
            {
                "subtitle": "set that our model is able to understand",
                "start": 1758.64,
                "dur": 2.96
            },
            {
                "subtitle": "because it needs to be in a format that",
                "start": 1760.32,
                "dur": 2.4
            },
            {
                "subtitle": "our model is able to understand.",
                "start": 1761.6,
                "dur": 2.64
            },
            {
                "subtitle": "Otherwise, it&#39;s impossible for the model",
                "start": 1762.72,
                "dur": 3.679
            },
            {
                "subtitle": "to basically convert random text files",
                "start": 1764.24,
                "dur": 4.159
            },
            {
                "subtitle": "into something to fine-tune. So look at",
                "start": 1766.399,
                "dur": 4.88
            },
            {
                "subtitle": "this. So this is basically all the",
                "start": 1768.399,
                "dur": 4.561
            },
            {
                "subtitle": "different ids. This has been broken down",
                "start": 1771.279,
                "dur": 3.441
            },
            {
                "subtitle": "into individual tokens. It can be had",
                "start": 1772.96,
                "dur": 4
            },
            {
                "subtitle": "taken refuge or it can be had taken as",
                "start": 1774.72,
                "dur": 4
            },
            {
                "subtitle": "one token, refugee second token in the",
                "start": 1776.96,
                "dur": 4.4
            },
            {
                "subtitle": "as a third token and whatever it has its",
                "start": 1778.72,
                "dur": 4.64
            },
            {
                "subtitle": "own vocabulary and based on that it is",
                "start": 1781.36,
                "dur": 4.16
            },
            {
                "subtitle": "able to convert this into the input ID.",
                "start": 1783.36,
                "dur": 3.679
            },
            {
                "subtitle": "So these are all the different input ids",
                "start": 1785.52,
                "dur": 3.2
            },
            {
                "subtitle": "and this is the attention mask. So what",
                "start": 1787.039,
                "dur": 4.481
            },
            {
                "subtitle": "do I mean by the attention mask? So this",
                "start": 1788.72,
                "dur": 5.839
            },
            {
                "subtitle": "attention mask means that this um input",
                "start": 1791.52,
                "dur": 5.279
            },
            {
                "subtitle": "ID specifically um input ID is basically",
                "start": 1794.559,
                "dur": 3.921
            },
            {
                "subtitle": "a token. Okay. So don&#39;t don&#39;t worry",
                "start": 1796.799,
                "dur": 2.801
            },
            {
                "subtitle": "about it. Don&#39;t think it&#39;s something",
                "start": 1798.48,
                "dur": 2.559
            },
            {
                "subtitle": "else. So this is basically the token",
                "start": 1799.6,
                "dur": 3.439
            },
            {
                "subtitle": "that will be used and this is the",
                "start": 1801.039,
                "dur": 4
            },
            {
                "subtitle": "numerical representation of the text. So",
                "start": 1803.039,
                "dur": 4.561
            },
            {
                "subtitle": "hat taken is represented as one. Refuge",
                "start": 1805.039,
                "dur": 5.36
            },
            {
                "subtitle": "is represented as 750 based on the",
                "start": 1807.6,
                "dur": 4.72
            },
            {
                "subtitle": "vocabulary of this model and this",
                "start": 1810.399,
                "dur": 4.4
            },
            {
                "subtitle": "tokenizer. And then the attention mask",
                "start": 1812.32,
                "dur": 4.56
            },
            {
                "subtitle": "is basically all at are one because it",
                "start": 1814.799,
                "dur": 4.081
            },
            {
                "subtitle": "has not been padded. So suppose there&#39;s",
                "start": 1816.88,
                "dur": 4.56
            },
            {
                "subtitle": "a token that has been padded. Let&#39;s see",
                "start": 1818.88,
                "dur": 5.6
            },
            {
                "subtitle": "an example. You will see the pad token",
                "start": 1821.44,
                "dur": 5.32
            },
            {
                "subtitle": "at the end and in that",
                "start": 1824.48,
                "dur": 5.679
            },
            {
                "subtitle": "case this is also not padded. But if you",
                "start": 1826.76,
                "dur": 5.639
            },
            {
                "subtitle": "if you&#39;re padding a basically data set",
                "start": 1830.159,
                "dur": 4
            },
            {
                "subtitle": "or a line then the attention mask will",
                "start": 1832.399,
                "dur": 3.841
            },
            {
                "subtitle": "all be zero after that and it will be",
                "start": 1834.159,
                "dur": 3.681
            },
            {
                "subtitle": "always a pad token. So we don&#39;t know the",
                "start": 1836.24,
                "dur": 4.319
            },
            {
                "subtitle": "pad token exactly what the token is.",
                "start": 1837.84,
                "dur": 4.88
            },
            {
                "subtitle": "It&#39;s the EOS token. So let let&#39;s see",
                "start": 1840.559,
                "dur": 5.921
            },
            {
                "subtitle": "let&#39;s see what it looks like. EOS token",
                "start": 1842.72,
                "dur": 5.679
            },
            {
                "subtitle": "do do like your own experiments because",
                "start": 1846.48,
                "dur": 3.439
            },
            {
                "subtitle": "otherwise it&#39;s very difficult for you to",
                "start": 1848.399,
                "dur": 3.201
            },
            {
                "subtitle": "understand everything based on like",
                "start": 1849.919,
                "dur": 3.76
            },
            {
                "subtitle": "listening knowledge or theory. This is",
                "start": 1851.6,
                "dur": 5.04
            },
            {
                "subtitle": "the best way to learn. So tokenizer us",
                "start": 1853.679,
                "dur": 4.641
            },
            {
                "subtitle": "token is this. So this is the pat token",
                "start": 1856.64,
                "dur": 3.68
            },
            {
                "subtitle": "as well. So this here you will see all",
                "start": 1858.32,
                "dur": 4.4
            },
            {
                "subtitle": "this based on numerical and then the",
                "start": 1860.32,
                "dur": 3.479
            },
            {
                "subtitle": "tension",
                "start": 1862.72,
                "dur": 3.76
            },
            {
                "subtitle": "mask. Sounds good. Now we are going to",
                "start": 1863.799,
                "dur": 4.76
            },
            {
                "subtitle": "do the model gradient checkpoint. Now",
                "start": 1866.48,
                "dur": 3.6
            },
            {
                "subtitle": "all the part comes where we&#39;re going to",
                "start": 1868.559,
                "dur": 3.041
            },
            {
                "subtitle": "train the model because we have loaded",
                "start": 1870.08,
                "dur": 3.839
            },
            {
                "subtitle": "the uh tokenizer and we have converted",
                "start": 1871.6,
                "dur": 5.12
            },
            {
                "subtitle": "our model uh our data set into tokens",
                "start": 1873.919,
                "dur": 4.24
            },
            {
                "subtitle": "and then we&#39;re all ready to go. We have",
                "start": 1876.72,
                "dur": 3.52
            },
            {
                "subtitle": "the data set, we have the tokenizer. Now",
                "start": 1878.159,
                "dur": 3.721
            },
            {
                "subtitle": "we just need the",
                "start": 1880.24,
                "dur": 3.919
            },
            {
                "subtitle": "model. So this is going to be",
                "start": 1881.88,
                "dur": 4.919
            },
            {
                "subtitle": "checkpointing dot enable and then model",
                "start": 1884.159,
                "dur": 4.441
            },
            {
                "subtitle": "is going to",
                "start": 1886.799,
                "dur": 6.681
            },
            {
                "subtitle": "be prepare model for",
                "start": 1888.6,
                "dur": 9.559
            },
            {
                "subtitle": "kbit training model.",
                "start": 1893.48,
                "dur": 7
            },
            {
                "subtitle": "Now it&#39;s going to be config equal to",
                "start": 1898.159,
                "dur": 3.88
            },
            {
                "subtitle": "lura",
                "start": 1900.48,
                "dur": 4.319
            },
            {
                "subtitle": "config. So lura config is going to be r",
                "start": 1902.039,
                "dur": 5.401
            },
            {
                "subtitle": "equal to0. I&#39;m going to talk about every",
                "start": 1904.799,
                "dur": 5.681
            },
            {
                "subtitle": "of these um parameters in detail. So let",
                "start": 1907.44,
                "dur": 5.839
            },
            {
                "subtitle": "me just complete it. Loa alpha is going",
                "start": 1910.48,
                "dur": 6.24
            },
            {
                "subtitle": "to be not zero but 64 and then the",
                "start": 1913.279,
                "dur": 6.041
            },
            {
                "subtitle": "target models are going to be q",
                "start": 1916.72,
                "dur": 6.559
            },
            {
                "subtitle": "proj um gate projection. I I&#39;ll I&#39;ll",
                "start": 1919.32,
                "dur": 5.64
            },
            {
                "subtitle": "discuss it. I&#39;ll discuss this in great",
                "start": 1923.279,
                "dur": 3.841
            },
            {
                "subtitle": "detail. So so that you can able to",
                "start": 1924.96,
                "dur": 4.959
            },
            {
                "subtitle": "understand this wait um approach and",
                "start": 1927.12,
                "dur": 6.48
            },
            {
                "subtitle": "then opro yeah we have total seven and",
                "start": 1929.919,
                "dur": 8.961
            },
            {
                "subtitle": "then bias um bias is going to be",
                "start": 1933.6,
                "dur": 5.28
            },
            {
                "subtitle": "none bias is going to be",
                "start": 1940.519,
                "dur": 7.081
            },
            {
                "subtitle": "none and",
                "start": 1944.36,
                "dur": 3.24
            },
            {
                "subtitle": "then load dropout is going to be 0.05 05",
                "start": 1948.519,
                "dur": 5.801
            },
            {
                "subtitle": "and then the task type is going to be",
                "start": 1952.24,
                "dur": 4.88
            },
            {
                "subtitle": "causal lm. So the auto complete is not",
                "start": 1954.32,
                "dur": 5.839
            },
            {
                "subtitle": "really good here but yeah so this is the",
                "start": 1957.12,
                "dur": 5.72
            },
            {
                "subtitle": "get pfg model and then model and then",
                "start": 1960.159,
                "dur": 5.561
            },
            {
                "subtitle": "config. Okay, sounds",
                "start": 1962.84,
                "dur": 8.439
            },
            {
                "subtitle": "good. I think I did some issue model",
                "start": 1965.72,
                "dur": 9.48
            },
            {
                "subtitle": "gradient checkpointing dot",
                "start": 1971.279,
                "dur": 3.921
            },
            {
                "subtitle": "enable. Okay. So again I would suggest",
                "start": 1976.919,
                "dur": 6.521
            },
            {
                "subtitle": "you to just use explain error here in",
                "start": 1980,
                "dur": 6.32
            },
            {
                "subtitle": "Gemini or you can use it on chat GPT",
                "start": 1983.44,
                "dur": 5.44
            },
            {
                "subtitle": "whatever you want but it&#39;s all okay. So",
                "start": 1986.32,
                "dur": 7.12
            },
            {
                "subtitle": "here the issue I know is that it&#39;s going",
                "start": 1988.88,
                "dur": 7
            },
            {
                "subtitle": "to be",
                "start": 1993.44,
                "dur": 5.359
            },
            {
                "subtitle": "checkpoint. Yeah this you can do either",
                "start": 1995.88,
                "dur": 6.639
            },
            {
                "subtitle": "or you can do checkpointing and both",
                "start": 1998.799,
                "dur": 8.6
            },
            {
                "subtitle": "works. and then um okay sorry this is",
                "start": 2002.519,
                "dur": 8.681
            },
            {
                "subtitle": "not zero this should be eight okay so",
                "start": 2007.399,
                "dur": 6.361
            },
            {
                "subtitle": "let&#39;s dive into the details of each and",
                "start": 2011.2,
                "dur": 5.64
            },
            {
                "subtitle": "every parameter here um",
                "start": 2013.76,
                "dur": 7.12
            },
            {
                "subtitle": "okay okay so r equal to0 r equal to 8 so",
                "start": 2016.84,
                "dur": 5.719
            },
            {
                "subtitle": "r equal to 8 is a good balance for most",
                "start": 2020.88,
                "dur": 4.639
            },
            {
                "subtitle": "tasks because a lower r means that it&#39;s",
                "start": 2022.559,
                "dur": 5.041
            },
            {
                "subtitle": "going to be less expressive and",
                "start": 2025.519,
                "dur": 4.561
            },
            {
                "subtitle": "basically r equal to 8 means update",
                "start": 2027.6,
                "dur": 4.48
            },
            {
                "subtitle": "matrices are rank eight so what do I",
                "start": 2030.08,
                "dur": 4
            },
            {
                "subtitle": "mean by that so let let Let us see the",
                "start": 2032.08,
                "dur": 4.64
            },
            {
                "subtitle": "transformer example. So then we&#39;ll be",
                "start": 2034.08,
                "dur": 4.64
            },
            {
                "subtitle": "able to understand better. So the highle",
                "start": 2036.72,
                "dur": 4.319
            },
            {
                "subtitle": "transformer block structure is this. So",
                "start": 2038.72,
                "dur": 4
            },
            {
                "subtitle": "here you have the input text and then",
                "start": 2041.039,
                "dur": 4.48
            },
            {
                "subtitle": "you have the tension block and inside",
                "start": 2042.72,
                "dur": 4.72
            },
            {
                "subtitle": "the tension block you have two layers.",
                "start": 2045.519,
                "dur": 3.921
            },
            {
                "subtitle": "So you have the multi head attention and",
                "start": 2047.44,
                "dur": 4.32
            },
            {
                "subtitle": "then which feeds the output into the",
                "start": 2049.44,
                "dur": 4.8
            },
            {
                "subtitle": "feed forward network or FFN. So let&#39;s",
                "start": 2051.76,
                "dur": 4.399
            },
            {
                "subtitle": "let&#39;s dive into the multi head attention",
                "start": 2054.24,
                "dur": 4.159
            },
            {
                "subtitle": "now. So this is the multi head attention",
                "start": 2056.159,
                "dur": 4
            },
            {
                "subtitle": "detail. So you have the input, you have",
                "start": 2058.399,
                "dur": 3.28
            },
            {
                "subtitle": "the linear layers, now we have the",
                "start": 2060.159,
                "dur": 4.401
            },
            {
                "subtitle": "query, key and value. So these are the",
                "start": 2061.679,
                "dur": 4.4
            },
            {
                "subtitle": "ones that are targeted because it&#39;s very",
                "start": 2064.56,
                "dur": 2.799
            },
            {
                "subtitle": "important. And next is the attention",
                "start": 2066.079,
                "dur": 2.8
            },
            {
                "subtitle": "scores. And then it now goes through the",
                "start": 2067.359,
                "dur": 3.28
            },
            {
                "subtitle": "O projection which is the output",
                "start": 2068.879,
                "dur": 3.2
            },
            {
                "subtitle": "projection. So this is also targeted",
                "start": 2070.639,
                "dur": 3.361
            },
            {
                "subtitle": "because these are the like it targets",
                "start": 2072.079,
                "dur": 2.921
            },
            {
                "subtitle": "basically",
                "start": 2074,
                "dur": 4.879
            },
            {
                "subtitle": "the update or rank matrices of specific",
                "start": 2075,
                "dur": 6.04
            },
            {
                "subtitle": "layers which are the most important. So",
                "start": 2078.879,
                "dur": 4.48
            },
            {
                "subtitle": "what do we mean by the query projection",
                "start": 2081.04,
                "dur": 5.44
            },
            {
                "subtitle": "layer? So let&#39;s discuss that.",
                "start": 2083.359,
                "dur": 6.841
            },
            {
                "subtitle": "So query projection layer is",
                "start": 2086.48,
                "dur": 6.24
            },
            {
                "subtitle": "basically the projects the input into",
                "start": 2090.2,
                "dur": 4.439
            },
            {
                "subtitle": "the question space. So for example let&#39;s",
                "start": 2092.72,
                "dur": 4.159
            },
            {
                "subtitle": "say we have what happened in Hawaii. It",
                "start": 2094.639,
                "dur": 4.241
            },
            {
                "subtitle": "becomes a query vector. So now the",
                "start": 2096.879,
                "dur": 4.72
            },
            {
                "subtitle": "original is going to be suppose 768 +",
                "start": 2098.88,
                "dur": 5.68
            },
            {
                "subtitle": "768 matrix and with Laura it&#39;s going to",
                "start": 2101.599,
                "dur": 7.601
            },
            {
                "subtitle": "be 2 768 + 8 and 8 + 768 matrix because",
                "start": 2104.56,
                "dur": 7.36
            },
            {
                "subtitle": "we have R equal to 8 is the rank. So",
                "start": 2109.2,
                "dur": 4.24
            },
            {
                "subtitle": "this way this is broken down into",
                "start": 2111.92,
                "dur": 3.439
            },
            {
                "subtitle": "smaller matrices because large matrices",
                "start": 2113.44,
                "dur": 3.679
            },
            {
                "subtitle": "are very difficult to deal with. So this",
                "start": 2115.359,
                "dur": 3.441
            },
            {
                "subtitle": "breaks it down to smaller matrices. Try",
                "start": 2117.119,
                "dur": 2.96
            },
            {
                "subtitle": "to understand this is very very",
                "start": 2118.8,
                "dur": 3.039
            },
            {
                "subtitle": "important and this will clear everything",
                "start": 2120.079,
                "dur": 4
            },
            {
                "subtitle": "you have doubt in. Now the key",
                "start": 2121.839,
                "dur": 3.921
            },
            {
                "subtitle": "projection or K projection. So this",
                "start": 2124.079,
                "dur": 3.52
            },
            {
                "subtitle": "projects the input to answer lookup",
                "start": 2125.76,
                "dur": 3.52
            },
            {
                "subtitle": "space. So what do I mean by that? So",
                "start": 2127.599,
                "dur": 3.201
            },
            {
                "subtitle": "basically it transforms the text into",
                "start": 2129.28,
                "dur": 2.6
            },
            {
                "subtitle": "searchable",
                "start": 2130.8,
                "dur": 3.44
            },
            {
                "subtitle": "keys. So basically suppose we have what",
                "start": 2131.88,
                "dur": 3.959
            },
            {
                "subtitle": "happens in Hawaii. Now it knows that",
                "start": 2134.24,
                "dur": 3.28
            },
            {
                "subtitle": "Hawaii is connected to this this this",
                "start": 2135.839,
                "dur": 4.24
            },
            {
                "subtitle": "this. So this is like a key key value",
                "start": 2137.52,
                "dur": 3.92
            },
            {
                "subtitle": "pair right? So you have Hawaii and",
                "start": 2140.079,
                "dur": 3.441
            },
            {
                "subtitle": "Hawaii is project projected to the",
                "start": 2141.44,
                "dur": 4.32
            },
            {
                "subtitle": "wildfire. It has projected to the forest",
                "start": 2143.52,
                "dur": 4.28
            },
            {
                "subtitle": "whatever it is. So you have a key lookup",
                "start": 2145.76,
                "dur": 4.48
            },
            {
                "subtitle": "pair. So and the next thing is the value",
                "start": 2147.8,
                "dur": 4.12
            },
            {
                "subtitle": "projection. So the value projection is",
                "start": 2150.24,
                "dur": 3.28
            },
            {
                "subtitle": "basically it projects the input to the",
                "start": 2151.92,
                "dur": 4
            },
            {
                "subtitle": "answer space and it transforms the text",
                "start": 2153.52,
                "dur": 4.48
            },
            {
                "subtitle": "into actual information that will help",
                "start": 2155.92,
                "dur": 4.88
            },
            {
                "subtitle": "us. So let so let&#39;s discuss let&#39;s",
                "start": 2158,
                "dur": 4.24
            },
            {
                "subtitle": "summarize. So first thing is query",
                "start": 2160.8,
                "dur": 3.6
            },
            {
                "subtitle": "projection. queries projects the query",
                "start": 2162.24,
                "dur": 3.599
            },
            {
                "subtitle": "what happened in Hawaii what is a",
                "start": 2164.4,
                "dur": 3.48
            },
            {
                "subtitle": "wildfire and all this into",
                "start": 2165.839,
                "dur": 4.881
            },
            {
                "subtitle": "a kind of token that is able that that",
                "start": 2167.88,
                "dur": 4.44
            },
            {
                "subtitle": "helps us to understand it that helps the",
                "start": 2170.72,
                "dur": 3.44
            },
            {
                "subtitle": "computer to understand it next thing is",
                "start": 2172.32,
                "dur": 3.44
            },
            {
                "subtitle": "the K projection or the key projection",
                "start": 2174.16,
                "dur": 3.199
            },
            {
                "subtitle": "it transforms the text into searchable",
                "start": 2175.76,
                "dur": 4.4
            },
            {
                "subtitle": "keys and third thing is the V projection",
                "start": 2177.359,
                "dur": 5.201
            },
            {
                "subtitle": "or value projection which transforms the",
                "start": 2180.16,
                "dur": 4.08
            },
            {
                "subtitle": "actual gives me the answer to the",
                "start": 2182.56,
                "dur": 3.76
            },
            {
                "subtitle": "problem and now the output projection it",
                "start": 2184.24,
                "dur": 3.68
            },
            {
                "subtitle": "basically combines and transforms and",
                "start": 2186.32,
                "dur": 3.84
            },
            {
                "subtitle": "filters the output such a way that we",
                "start": 2187.92,
                "dur": 3.76
            },
            {
                "subtitle": "are able to understand in our own",
                "start": 2190.16,
                "dur": 3.52
            },
            {
                "subtitle": "language Next thing is the feed forward",
                "start": 2191.68,
                "dur": 4.159
            },
            {
                "subtitle": "network. So it has the last three layers",
                "start": 2193.68,
                "dur": 3.6
            },
            {
                "subtitle": "the gate projection the up projection",
                "start": 2195.839,
                "dur": 2.801
            },
            {
                "subtitle": "and the down projection. So gate",
                "start": 2197.28,
                "dur": 3.04
            },
            {
                "subtitle": "projection it controls the information",
                "start": 2198.64,
                "dur": 3.92
            },
            {
                "subtitle": "flow acts like a smart filter. There are",
                "start": 2200.32,
                "dur": 4.48
            },
            {
                "subtitle": "guardrails there are this not to be said",
                "start": 2202.56,
                "dur": 4
            },
            {
                "subtitle": "there are some kind of passwords the",
                "start": 2204.8,
                "dur": 3.36
            },
            {
                "subtitle": "these are suppose these are not to be",
                "start": 2206.56,
                "dur": 3.92
            },
            {
                "subtitle": "shared at all. So this is the gate",
                "start": 2208.16,
                "dur": 3.76
            },
            {
                "subtitle": "projection. Next thing is the up",
                "start": 2210.48,
                "dur": 2.96
            },
            {
                "subtitle": "projection which means expands the",
                "start": 2211.92,
                "dur": 3.04
            },
            {
                "subtitle": "information and we also need a down",
                "start": 2213.44,
                "dur": 3.04
            },
            {
                "subtitle": "projection because we are able to",
                "start": 2214.96,
                "dur": 3.52
            },
            {
                "subtitle": "control these parameters based on what",
                "start": 2216.48,
                "dur": 4.24
            },
            {
                "subtitle": "our data set looks like. This is very",
                "start": 2218.48,
                "dur": 3.92
            },
            {
                "subtitle": "important. And this basically clears",
                "start": 2220.72,
                "dur": 3.76
            },
            {
                "subtitle": "everything that you have doubt in",
                "start": 2222.4,
                "dur": 4.24
            },
            {
                "subtitle": "because we have up projection, gate",
                "start": 2224.48,
                "dur": 4
            },
            {
                "subtitle": "projection, upward projection, downward",
                "start": 2226.64,
                "dur": 4.8
            },
            {
                "subtitle": "projection. We have um Q projection,",
                "start": 2228.48,
                "dur": 5.119
            },
            {
                "subtitle": "query projection, key projection and",
                "start": 2231.44,
                "dur": 3.6
            },
            {
                "subtitle": "value projection. We have an",
                "start": 2233.599,
                "dur": 3.361
            },
            {
                "subtitle": "understanding of every one of these",
                "start": 2235.04,
                "dur": 5.039
            },
            {
                "subtitle": "seven projections. That&#39;s it basically.",
                "start": 2236.96,
                "dur": 5.44
            },
            {
                "subtitle": "That&#39;s it. That that is enough for your",
                "start": 2240.079,
                "dur": 3.52
            },
            {
                "subtitle": "interview and everything that you need",
                "start": 2242.4,
                "dur": 2.959
            },
            {
                "subtitle": "to know even for research purposes as",
                "start": 2243.599,
                "dur": 6.52
            },
            {
                "subtitle": "well. This is enough and okay.",
                "start": 2245.359,
                "dur": 7.361
            },
            {
                "subtitle": "So, okay. So, great. So, now we have the",
                "start": 2250.119,
                "dur": 4.681
            },
            {
                "subtitle": "PFT model. We know what everything does",
                "start": 2252.72,
                "dur": 4.639
            },
            {
                "subtitle": "here. And now we are going to train the",
                "start": 2254.8,
                "dur": 6.08
            },
            {
                "subtitle": "model. Let&#39;s do that. Transformers uh",
                "start": 2257.359,
                "dur": 7.921
            },
            {
                "subtitle": "sorry, trainer equal",
                "start": 2260.88,
                "dur": 4.4
            },
            {
                "subtitle": "to transformers",
                "start": 2265.48,
                "dur": 6.76
            },
            {
                "subtitle": "dot trainer. And this is going to be",
                "start": 2269.48,
                "dur": 5.4
            },
            {
                "subtitle": "model equal to model. And then it&#39;s",
                "start": 2272.24,
                "dur": 4.32
            },
            {
                "subtitle": "going to be train data set equal to",
                "start": 2274.88,
                "dur": 4
            },
            {
                "subtitle": "organized train data set. is going to be",
                "start": 2276.56,
                "dur": 5.72
            },
            {
                "subtitle": "args equal to transformers dot training",
                "start": 2278.88,
                "dur": 8.199
            },
            {
                "subtitle": "arguments and then we need the output",
                "start": 2282.28,
                "dur": 8
            },
            {
                "subtitle": "directory as dot",
                "start": 2287.079,
                "dur": 5.361
            },
            {
                "subtitle": "/finetuned",
                "start": 2290.28,
                "dur": 4.52
            },
            {
                "subtitle": "model. Now it&#39;s going to be per device",
                "start": 2292.44,
                "dur": 4.76
            },
            {
                "subtitle": "train size as let&#39;s say it&#39;s two to make",
                "start": 2294.8,
                "dur": 4.24
            },
            {
                "subtitle": "it faster. This is a bad size like if",
                "start": 2297.2,
                "dur": 4.879
            },
            {
                "subtitle": "it&#39;s larger size then 2 to22 will be 222",
                "start": 2299.04,
                "dur": 4.88
            },
            {
                "subtitle": "will be done in each iteration otherwise",
                "start": 2302.079,
                "dur": 4.801
            },
            {
                "subtitle": "it&#39;s yeah whatever it can be one. And",
                "start": 2303.92,
                "dur": 6.08
            },
            {
                "subtitle": "next thing is num train epo total epox",
                "start": 2306.88,
                "dur": 5.44
            },
            {
                "subtitle": "total iterations we want to run it for",
                "start": 2310,
                "dur": 4.68
            },
            {
                "subtitle": "total we want to run it for three we",
                "start": 2312.32,
                "dur": 8.88
            },
            {
                "subtitle": "have learning rate as 1 e - 4 10 ^ -",
                "start": 2314.68,
                "dur": 6.52
            },
            {
                "subtitle": "4 minus 4 and we have max",
                "start": 2322.68,
                "dur": 7.159
            },
            {
                "subtitle": "steps max steps I don&#39;t want to run it",
                "start": 2327.24,
                "dur": 5.08
            },
            {
                "subtitle": "more than 20 but listen to me you will",
                "start": 2329.839,
                "dur": 6.081
            },
            {
                "subtitle": "be running it for at least 1500 steps",
                "start": 2332.32,
                "dur": 6.24
            },
            {
                "subtitle": "Just let it be there, run there for some",
                "start": 2335.92,
                "dur": 4.96
            },
            {
                "subtitle": "time because I want to finish it fast.",
                "start": 2338.56,
                "dur": 4.559
            },
            {
                "subtitle": "That is why I&#39;m doing it 20. But you",
                "start": 2340.88,
                "dur": 4.32
            },
            {
                "subtitle": "need to do it at least for,000 steps or",
                "start": 2343.119,
                "dur": 4.081
            },
            {
                "subtitle": "1500 steps to actually see valuable",
                "start": 2345.2,
                "dur": 5.04
            },
            {
                "subtitle": "things coming out of it. Okay. So now",
                "start": 2347.2,
                "dur": 6.8
            },
            {
                "subtitle": "next thing is BF16 is going to be false.",
                "start": 2350.24,
                "dur": 6.32
            },
            {
                "subtitle": "Here we&#39;re not going to use BF16. I mean",
                "start": 2354,
                "dur": 5.04
            },
            {
                "subtitle": "you can use BF16 but we want to see what",
                "start": 2356.56,
                "dur": 4.16
            },
            {
                "subtitle": "happens without BF16 as well because",
                "start": 2359.04,
                "dur": 3.44
            },
            {
                "subtitle": "we&#39;re already doing double quantization",
                "start": 2360.72,
                "dur": 5.48
            },
            {
                "subtitle": "before as I showed you. remember",
                "start": 2362.48,
                "dur": 6.16
            },
            {
                "subtitle": "um yeah already using double",
                "start": 2366.2,
                "dur": 5.08
            },
            {
                "subtitle": "quantization so it&#39;s fine at the end to",
                "start": 2368.64,
                "dur": 5.479
            },
            {
                "subtitle": "not use PF16 to use a standard",
                "start": 2371.28,
                "dur": 5.92
            },
            {
                "subtitle": "FP32 now optim is going to be the this",
                "start": 2374.119,
                "dur": 4.841
            },
            {
                "subtitle": "is a bit this is a new thing so I&#39;m",
                "start": 2377.2,
                "dur": 4.04
            },
            {
                "subtitle": "going to discuss this a little",
                "start": 2378.96,
                "dur": 6.56
            },
            {
                "subtitle": "bit 8 bit something you can use or you",
                "start": 2381.24,
                "dur": 7.48
            },
            {
                "subtitle": "cannot use whatever whatever it&#39;s fine",
                "start": 2385.52,
                "dur": 4.96
            },
            {
                "subtitle": "and now it&#39;s coming to be the login",
                "start": 2388.72,
                "dur": 4.56
            },
            {
                "subtitle": "directory to be the log and you have the",
                "start": 2390.48,
                "dur": 6.44
            },
            {
                "subtitle": "save strategy as",
                "start": 2393.28,
                "dur": 6.36
            },
            {
                "subtitle": "epoch and you have the",
                "start": 2396.92,
                "dur": 7.32
            },
            {
                "subtitle": "save steps as 50. It&#39;s fine. If you&#39;re",
                "start": 2399.64,
                "dur": 7.32
            },
            {
                "subtitle": "doing for 1500 or 2,000 data points,",
                "start": 2404.24,
                "dur": 5.76
            },
            {
                "subtitle": "then it&#39;s very important to do this to",
                "start": 2406.96,
                "dur": 4.56
            },
            {
                "subtitle": "50 because you want to see what&#39;s going",
                "start": 2410,
                "dur": 3.28
            },
            {
                "subtitle": "on, whether loss is decreasing, loss is",
                "start": 2411.52,
                "dur": 2.8
            },
            {
                "subtitle": "increasing, whether you need to change",
                "start": 2413.28,
                "dur": 2.64
            },
            {
                "subtitle": "any kind of other hyperparameters or",
                "start": 2414.32,
                "dur": 3.4
            },
            {
                "subtitle": "not. But",
                "start": 2415.92,
                "dur": 4.8
            },
            {
                "subtitle": "yeah and then you have this and then the",
                "start": 2417.72,
                "dur": 5.24
            },
            {
                "subtitle": "next thing is you need the data",
                "start": 2420.72,
                "dur": 4.16
            },
            {
                "subtitle": "collector. So this is something I&#39;ll",
                "start": 2422.96,
                "dur": 3.68
            },
            {
                "subtitle": "need to explain to you in greater",
                "start": 2424.88,
                "dur": 5.92
            },
            {
                "subtitle": "detail. So oh be ready.",
                "start": 2426.64,
                "dur": 5.92
            },
            {
                "subtitle": "So I&#39;ll just finish this and then we",
                "start": 2430.8,
                "dur": 5.6
            },
            {
                "subtitle": "will discuss data",
                "start": 2432.56,
                "dur": 3.84
            },
            {
                "subtitle": "collage model link",
                "start": 2436.76,
                "dur": 6.76
            },
            {
                "subtitle": "as",
                "start": 2440.52,
                "dur": 3
            },
            {
                "subtitle": "tokenizer MLM equal to",
                "start": 2443.72,
                "dur": 4.04
            },
            {
                "subtitle": "false and then we have the normal thing",
                "start": 2448.599,
                "dur": 9.76
            },
            {
                "subtitle": "like model config dot use cache equal to",
                "start": 2453.16,
                "dur": 9.04
            },
            {
                "subtitle": "false and And then we have the trainer",
                "start": 2458.359,
                "dur": 6.361
            },
            {
                "subtitle": "train. So I&#39;m going to let it running",
                "start": 2462.2,
                "dur": 4.68
            },
            {
                "subtitle": "and I&#39;m going to explain these all these",
                "start": 2464.72,
                "dur": 7.359
            },
            {
                "subtitle": "in great detail and I did a spelling",
                "start": 2466.88,
                "dur": 5.199
            },
            {
                "subtitle": "mistake. Okay, sounds good. Um, okay.",
                "start": 2475.16,
                "dur": 6.52
            },
            {
                "subtitle": "Model config is not defined. Um, sorry,",
                "start": 2478.079,
                "dur": 6.161
            },
            {
                "subtitle": "it&#39;s going to be",
                "start": 2481.68,
                "dur": 2.56
            },
            {
                "subtitle": "model.config. That&#39;s all right. Okay,",
                "start": 2484.359,
                "dur": 3.641
            },
            {
                "subtitle": "now let&#39;s discuss let&#39;s discuss all this",
                "start": 2486.319,
                "dur": 4.641
            },
            {
                "subtitle": "in detail.",
                "start": 2488,
                "dur": 2.96
            },
            {
                "subtitle": "Okay. So definitely the model you can",
                "start": 2492.68,
                "dur": 3.56
            },
            {
                "subtitle": "understand then the data set you can",
                "start": 2495.04,
                "dur": 2.799
            },
            {
                "subtitle": "understand then the bat size you know",
                "start": 2496.24,
                "dur": 3.119
            },
            {
                "subtitle": "two and gradient accumulation steps is",
                "start": 2497.839,
                "dur": 2.801
            },
            {
                "subtitle": "two. So remember this gradient",
                "start": 2499.359,
                "dur": 3.281
            },
            {
                "subtitle": "accumulation step is basically batch",
                "start": 2500.64,
                "dur": 4.4
            },
            {
                "subtitle": "processing flow. So it&#39;s basically the",
                "start": 2502.64,
                "dur": 3.92
            },
            {
                "subtitle": "how we are processing the entire data",
                "start": 2505.04,
                "dur": 3.44
            },
            {
                "subtitle": "set. So basically in the iteration one",
                "start": 2506.56,
                "dur": 3.44
            },
            {
                "subtitle": "we&#39;re going to do a forward pass and",
                "start": 2508.48,
                "dur": 2.8
            },
            {
                "subtitle": "we&#39;re going to have the gradient",
                "start": 2510,
                "dur": 3.04
            },
            {
                "subtitle": "calculated and stored and the second",
                "start": 2511.28,
                "dur": 3.12
            },
            {
                "subtitle": "iteration we&#39;re going to do again a",
                "start": 2513.04,
                "dur": 2.88
            },
            {
                "subtitle": "forward pass and then we have the again",
                "start": 2514.4,
                "dur": 3.679
            },
            {
                "subtitle": "the gradient calculated and then these",
                "start": 2515.92,
                "dur": 3.76
            },
            {
                "subtitle": "two gradients we&#39;re going to combine and",
                "start": 2518.079,
                "dur": 3.04
            },
            {
                "subtitle": "then do the model update. We&#39;re not",
                "start": 2519.68,
                "dur": 2.88
            },
            {
                "subtitle": "going to do it in between. So this is",
                "start": 2521.119,
                "dur": 2.881
            },
            {
                "subtitle": "what is called the gradient accumulation",
                "start": 2522.56,
                "dur": 3.2
            },
            {
                "subtitle": "steps is two. So it&#39;s going to remember",
                "start": 2524,
                "dur": 3.44
            },
            {
                "subtitle": "the gradient that calculated. It&#39;s going",
                "start": 2525.76,
                "dur": 3.839
            },
            {
                "subtitle": "to combine the two gradients to conduct",
                "start": 2527.44,
                "dur": 4.08
            },
            {
                "subtitle": "specific formula and then it&#39;s going to",
                "start": 2529.599,
                "dur": 3.041
            },
            {
                "subtitle": "update the model. So this is the",
                "start": 2531.52,
                "dur": 2.799
            },
            {
                "subtitle": "gradient accumulation steps. You can",
                "start": 2532.64,
                "dur": 4.24
            },
            {
                "subtitle": "change as you want it but it&#39;s better to",
                "start": 2534.319,
                "dur": 4.721
            },
            {
                "subtitle": "keep it as two because it&#39;s the most",
                "start": 2536.88,
                "dur": 4.56
            },
            {
                "subtitle": "standard and then you can you can play",
                "start": 2539.04,
                "dur": 3.76
            },
            {
                "subtitle": "with it. I would encourage you to play",
                "start": 2541.44,
                "dur": 3.44
            },
            {
                "subtitle": "with it because your interview mat might",
                "start": 2542.8,
                "dur": 4
            },
            {
                "subtitle": "ask you to do this to do that and it&#39;s",
                "start": 2544.88,
                "dur": 3.12
            },
            {
                "subtitle": "very important that you have a good",
                "start": 2546.8,
                "dur": 3.16
            },
            {
                "subtitle": "knowledge of this because this is our",
                "start": 2548,
                "dur": 4.4
            },
            {
                "subtitle": "future. And then comes the number train",
                "start": 2549.96,
                "dur": 3.96
            },
            {
                "subtitle": "box is three runs through the entire",
                "start": 2552.4,
                "dur": 3.36
            },
            {
                "subtitle": "data set three times and next is max",
                "start": 2553.92,
                "dur": 4.159
            },
            {
                "subtitle": "steps is 20. So it stops after 20 update",
                "start": 2555.76,
                "dur": 4.319
            },
            {
                "subtitle": "steps and then you have the learning",
                "start": 2558.079,
                "dur": 4
            },
            {
                "subtitle": "rate at 10 the minus 4 which means a",
                "start": 2560.079,
                "dur": 3.52
            },
            {
                "subtitle": "small learning rate for finetuning is",
                "start": 2562.079,
                "dur": 3.921
            },
            {
                "subtitle": "fine. You don&#39;t want to overfit and you",
                "start": 2563.599,
                "dur": 4.48
            },
            {
                "subtitle": "don&#39;t want to um you want it to be",
                "start": 2566,
                "dur": 5.44
            },
            {
                "subtitle": "studied incrementally and bloistine is",
                "start": 2568.079,
                "dur": 6.081
            },
            {
                "subtitle": "not being used that&#39;s okay. Now this is",
                "start": 2571.44,
                "dur": 6.56
            },
            {
                "subtitle": "very important optimum w optimizer is",
                "start": 2574.16,
                "dur": 6.64
            },
            {
                "subtitle": "paged Adamw 8 bit. So this is an memory",
                "start": 2578,
                "dur": 5.119
            },
            {
                "subtitle": "efficient Adamw optimizer. So standard",
                "start": 2580.8,
                "dur": 5.36
            },
            {
                "subtitle": "Adamw means that all the parameters and",
                "start": 2583.119,
                "dur": 5.041
            },
            {
                "subtitle": "optimizer states are in memory. But",
                "start": 2586.16,
                "dur": 4.52
            },
            {
                "subtitle": "paged 8 bit adamw means that model",
                "start": 2588.16,
                "dur": 5.439
            },
            {
                "subtitle": "parameters are basically like paged",
                "start": 2590.68,
                "dur": 5.08
            },
            {
                "subtitle": "optimizer states they are 8 bit",
                "start": 2593.599,
                "dur": 3.841
            },
            {
                "subtitle": "quantized states that are in memory.",
                "start": 2595.76,
                "dur": 3.28
            },
            {
                "subtitle": "It&#39;s not the entire thing is in memory.",
                "start": 2597.44,
                "dur": 2.72
            },
            {
                "subtitle": "We don&#39;t want the entire thing because",
                "start": 2599.04,
                "dur": 2.319
            },
            {
                "subtitle": "anyway we&#39;re not training it. We&#39;re not",
                "start": 2600.16,
                "dur": 3.12
            },
            {
                "subtitle": "loading it in the full memory and it&#39;s",
                "start": 2601.359,
                "dur": 3.841
            },
            {
                "subtitle": "not possible not needed. So you don&#39;t",
                "start": 2603.28,
                "dur": 4.48
            },
            {
                "subtitle": "want to waste things. So basically this",
                "start": 2605.2,
                "dur": 4.48
            },
            {
                "subtitle": "optimizer is for the 8 bit quantized",
                "start": 2607.76,
                "dur": 3.8
            },
            {
                "subtitle": "states and the rest is very simple to",
                "start": 2609.68,
                "dur": 4.24
            },
            {
                "subtitle": "understand. Now comes the data coller",
                "start": 2611.56,
                "dur": 3.559
            },
            {
                "subtitle": "which is very important. And the data",
                "start": 2613.92,
                "dur": 4.64
            },
            {
                "subtitle": "collator is basically what it does is it",
                "start": 2615.119,
                "dur": 6.24
            },
            {
                "subtitle": "has like an input sequence. So sequence",
                "start": 2618.56,
                "dur": 5.12
            },
            {
                "subtitle": "one has a lot of tokens. Total tokens is",
                "start": 2621.359,
                "dur": 4.72
            },
            {
                "subtitle": "suppose it&#39;s 50 and sequence 2 has a",
                "start": 2623.68,
                "dur": 4.48
            },
            {
                "subtitle": "total tokens as 30. So what collator",
                "start": 2626.079,
                "dur": 4
            },
            {
                "subtitle": "does is finds the max length which is",
                "start": 2628.16,
                "dur": 3.919
            },
            {
                "subtitle": "50. Basically the padding thing is done",
                "start": 2630.079,
                "dur": 4.561
            },
            {
                "subtitle": "by the data collator which is very very",
                "start": 2632.079,
                "dur": 4.481
            },
            {
                "subtitle": "very important. I cannot stress it",
                "start": 2634.64,
                "dur": 4
            },
            {
                "subtitle": "enough. This is important to have a",
                "start": 2636.56,
                "dur": 4.32
            },
            {
                "subtitle": "optimized data set which is of the same",
                "start": 2638.64,
                "dur": 4.24
            },
            {
                "subtitle": "length of the same width. not same",
                "start": 2640.88,
                "dur": 4
            },
            {
                "subtitle": "length of the same width throughout so",
                "start": 2642.88,
                "dur": 3.56
            },
            {
                "subtitle": "that it&#39;s able to learn everything",
                "start": 2644.88,
                "dur": 4.479
            },
            {
                "subtitle": "perfectly. So the same max length is",
                "start": 2646.44,
                "dur": 5.48
            },
            {
                "subtitle": "going to be 50 and now we&#39;re going to do",
                "start": 2649.359,
                "dur": 5.361
            },
            {
                "subtitle": "pad the shorter sequences. Basically I",
                "start": 2651.92,
                "dur": 4.56
            },
            {
                "subtitle": "showed you the input tokens are going to",
                "start": 2654.72,
                "dur": 3.92
            },
            {
                "subtitle": "be comma and then pad pad pad token",
                "start": 2656.48,
                "dur": 4
            },
            {
                "subtitle": "whatever it is and then the attention",
                "start": 2658.64,
                "dur": 3.52
            },
            {
                "subtitle": "mask is going to be 00 0 because it&#39;s",
                "start": 2660.48,
                "dur": 2.96
            },
            {
                "subtitle": "the padding thing we don&#39;t want it to",
                "start": 2662.16,
                "dur": 2.959
            },
            {
                "subtitle": "learn but we want the data set to be of",
                "start": 2663.44,
                "dur": 2.76
            },
            {
                "subtitle": "the same",
                "start": 2665.119,
                "dur": 3.361
            },
            {
                "subtitle": "width. I hope you understand this. So",
                "start": 2666.2,
                "dur": 4.36
            },
            {
                "subtitle": "this is the final complete training",
                "start": 2668.48,
                "dur": 4.32
            },
            {
                "subtitle": "flow. Remember this very well. So data",
                "start": 2670.56,
                "dur": 4.64
            },
            {
                "subtitle": "flow during training first raw text how",
                "start": 2672.8,
                "dur": 4.799
            },
            {
                "subtitle": "fires next thing is tokenization next",
                "start": 2675.2,
                "dur": 4.159
            },
            {
                "subtitle": "thing is data collision next thing is",
                "start": 2677.599,
                "dur": 4.401
            },
            {
                "subtitle": "patching then comes forward pass then is",
                "start": 2679.359,
                "dur": 4.72
            },
            {
                "subtitle": "the calculate loss and then is the",
                "start": 2682,
                "dur": 4.079
            },
            {
                "subtitle": "accumulate gradients and then update",
                "start": 2684.079,
                "dur": 3.681
            },
            {
                "subtitle": "weights and then save checkpoints.",
                "start": 2686.079,
                "dur": 3.24
            },
            {
                "subtitle": "That&#39;s it. You have the model and then",
                "start": 2687.76,
                "dur": 4.72
            },
            {
                "subtitle": "inference. Very very simple and very",
                "start": 2689.319,
                "dur": 7.241
            },
            {
                "subtitle": "very like great. Okay. I will need to",
                "start": 2692.48,
                "dur": 5.599
            },
            {
                "subtitle": "provide an API key. Okay, I&#39;m going to",
                "start": 2696.56,
                "dur": 3.84
            },
            {
                "subtitle": "blur this out but you just I have got my",
                "start": 2698.079,
                "dur": 4.321
            },
            {
                "subtitle": "API key. It&#39;s absolutely free. It helps",
                "start": 2700.4,
                "dur": 4.16
            },
            {
                "subtitle": "you to keep a track of what weights are",
                "start": 2702.4,
                "dur": 4
            },
            {
                "subtitle": "there, how much you have trained, what",
                "start": 2704.56,
                "dur": 4.799
            },
            {
                "subtitle": "is the current status and everything.",
                "start": 2706.4,
                "dur": 5.84
            },
            {
                "subtitle": "Great. I think we have everything here.",
                "start": 2709.359,
                "dur": 5.601
            },
            {
                "subtitle": "Now we just need the inference and then",
                "start": 2712.24,
                "dur": 5.28
            },
            {
                "subtitle": "we are done. So let&#39;s start writing the",
                "start": 2714.96,
                "dur": 4.48
            },
            {
                "subtitle": "inference as we are as calculating the",
                "start": 2717.52,
                "dur": 3.16
            },
            {
                "subtitle": "loss.",
                "start": 2719.44,
                "dur": 4.399
            },
            {
                "subtitle": "So I&#39;m going to write in a separate way",
                "start": 2720.68,
                "dur": 6.12
            },
            {
                "subtitle": "because we want we often have like in",
                "start": 2723.839,
                "dur": 4.881
            },
            {
                "subtitle": "Python files and we are doing it then we",
                "start": 2726.8,
                "dur": 4
            },
            {
                "subtitle": "have it in different files we have a",
                "start": 2728.72,
                "dur": 4.639
            },
            {
                "subtitle": "train.py and we have a test.py by",
                "start": 2730.8,
                "dur": 5.519
            },
            {
                "subtitle": "standard like nomen cle so that is why",
                "start": 2733.359,
                "dur": 5.121
            },
            {
                "subtitle": "I&#39;m making the import statements again",
                "start": 2736.319,
                "dur": 4.121
            },
            {
                "subtitle": "so we are going to",
                "start": 2738.48,
                "dur": 5.96
            },
            {
                "subtitle": "do import torch and then from",
                "start": 2740.44,
                "dur": 7.76
            },
            {
                "subtitle": "transformers import auto",
                "start": 2744.44,
                "dur": 10.399
            },
            {
                "subtitle": "tokenizer and auto model for causal",
                "start": 2748.2,
                "dur": 10
            },
            {
                "subtitle": "lm next thing is from",
                "start": 2754.839,
                "dur": 9.201
            },
            {
                "subtitle": "transformers import bits and byte It&#39;s",
                "start": 2758.2,
                "dur": 8.68
            },
            {
                "subtitle": "config llama tokenizer. Understand this",
                "start": 2764.04,
                "dur": 4.12
            },
            {
                "subtitle": "inference thing as well because if you",
                "start": 2766.88,
                "dur": 3.28
            },
            {
                "subtitle": "train a model and it&#39;s perfect, it is",
                "start": 2768.16,
                "dur": 3.6
            },
            {
                "subtitle": "great but you don&#39;t know how to",
                "start": 2770.16,
                "dur": 3.36
            },
            {
                "subtitle": "inference it then just throw it in the",
                "start": 2771.76,
                "dur": 6.92
            },
            {
                "subtitle": "dust bin from PFT import PFT",
                "start": 2773.52,
                "dur": 8.24
            },
            {
                "subtitle": "model. Next thing that comes is this",
                "start": 2778.68,
                "dur": 6.639
            },
            {
                "subtitle": "model ID is going to",
                "start": 2781.76,
                "dur": 6.559
            },
            {
                "subtitle": "be this is done. So obviously the losses",
                "start": 2785.319,
                "dur": 5
            },
            {
                "subtitle": "decrease but it can go a lot less",
                "start": 2788.319,
                "dur": 4.321
            },
            {
                "subtitle": "because I stopped it at 20. Just do it",
                "start": 2790.319,
                "dur": 3.52
            },
            {
                "subtitle": "till 1,000 and you will see the",
                "start": 2792.64,
                "dur": 2.439
            },
            {
                "subtitle": "difference",
                "start": 2793.839,
                "dur": 3.76
            },
            {
                "subtitle": "guaranteed. Base model ID is going to be",
                "start": 2795.079,
                "dur": 4.361
            },
            {
                "subtitle": "the same thing and I&#39;m not going to",
                "start": 2797.599,
                "dur": 2.921
            },
            {
                "subtitle": "write it",
                "start": 2799.44,
                "dur": 4.52
            },
            {
                "subtitle": "again. So this is the",
                "start": 2800.52,
                "dur": 5.64
            },
            {
                "subtitle": "thing. I&#39;m going to share all the links",
                "start": 2803.96,
                "dur": 3.879
            },
            {
                "subtitle": "in the description so don&#39;t worry about",
                "start": 2806.16,
                "dur": 6.12
            },
            {
                "subtitle": "it. And now it comes the NF4",
                "start": 2807.839,
                "dur": 7.441
            },
            {
                "subtitle": "config normal float 4 config cuz we&#39;re",
                "start": 2812.28,
                "dur": 4.839
            },
            {
                "subtitle": "doing it in 4bit quantization that is",
                "start": 2815.28,
                "dur": 5.88
            },
            {
                "subtitle": "why bitsson bytes",
                "start": 2817.119,
                "dur": 7.841
            },
            {
                "subtitle": "config now we have load in I really",
                "start": 2821.16,
                "dur": 5.4
            },
            {
                "subtitle": "don&#39;t think we need to write this again",
                "start": 2824.96,
                "dur": 4.159
            },
            {
                "subtitle": "let&#39;s copy",
                "start": 2826.56,
                "dur": 2.559
            },
            {
                "subtitle": "paste and we have it",
                "start": 2829.88,
                "dur": 4.12
            },
            {
                "subtitle": "There you go. We have load in 4 bit as",
                "start": 2838.24,
                "dur": 5.44
            },
            {
                "subtitle": "four. We have BNB 4 bit use double quant",
                "start": 2840.56,
                "dur": 5.68
            },
            {
                "subtitle": "is true. We have BNB 4 bit quant type as",
                "start": 2843.68,
                "dur": 5.2
            },
            {
                "subtitle": "NF4. And we have the data type as B416.",
                "start": 2846.24,
                "dur": 3.76
            },
            {
                "subtitle": "I hope you know this. I&#39;ve already",
                "start": 2848.88,
                "dur": 2.959
            },
            {
                "subtitle": "talked about it. So I think it&#39;s okay. I",
                "start": 2850,
                "dur": 3.2
            },
            {
                "subtitle": "think there&#39;s a spelling mistake I have",
                "start": 2851.839,
                "dur": 3.841
            },
            {
                "subtitle": "here. So this is very important. Now the",
                "start": 2853.2,
                "dur": 5.68
            },
            {
                "subtitle": "next thing is tokenizer equal to llama.",
                "start": 2855.68,
                "dur": 4.56
            },
            {
                "subtitle": "You have to do the entire same thing the",
                "start": 2858.88,
                "dur": 2.719
            },
            {
                "subtitle": "way you did the prep-processing there.",
                "start": 2860.24,
                "dur": 3.2
            },
            {
                "subtitle": "you&#39;re going to do it here because that",
                "start": 2861.599,
                "dur": 3.601
            },
            {
                "subtitle": "is how the model recognizes the text and",
                "start": 2863.44,
                "dur": 3.04
            },
            {
                "subtitle": "that is how the model is able to give",
                "start": 2865.2,
                "dur": 4.159
            },
            {
                "subtitle": "the answer.",
                "start": 2866.48,
                "dur": 2.879
            },
            {
                "subtitle": "Okay. Okay. Now we have the from",
                "start": 2869.8,
                "dur": 7.88
            },
            {
                "subtitle": "pre-trained and you have base model id",
                "start": 2873.8,
                "dur": 5.44
            },
            {
                "subtitle": "you have use",
                "start": 2877.68,
                "dur": 4.48
            },
            {
                "subtitle": "fast think we can use this let&#39;s see huh",
                "start": 2879.24,
                "dur": 5.32
            },
            {
                "subtitle": "yes add us token is true do the same",
                "start": 2882.16,
                "dur": 4.48
            },
            {
                "subtitle": "thing exactly the same thing that you",
                "start": 2884.56,
                "dur": 3.92
            },
            {
                "subtitle": "have done during training if you really",
                "start": 2886.64,
                "dur": 3.679
            },
            {
                "subtitle": "want your model to succeed well if you",
                "start": 2888.48,
                "dur": 3.92
            },
            {
                "subtitle": "don&#39;t then it&#39;s fine but I&#39;m just saying",
                "start": 2890.319,
                "dur": 4.081
            },
            {
                "subtitle": "that you need use fast is false and then",
                "start": 2892.4,
                "dur": 3.919
            },
            {
                "subtitle": "trust remote code as true and then add",
                "start": 2894.4,
                "dur": 5.28
            },
            {
                "subtitle": "US token as true now this is just auto",
                "start": 2896.319,
                "dur": 6.881
            },
            {
                "subtitle": "completed but yeah we can use it I think",
                "start": 2899.68,
                "dur": 5.72
            },
            {
                "subtitle": "so we can use",
                "start": 2903.2,
                "dur": 4.8
            },
            {
                "subtitle": "it base model is going to be auto model",
                "start": 2905.4,
                "dur": 4.919
            },
            {
                "subtitle": "for causal LM do from preent we have the",
                "start": 2908,
                "dur": 5.599
            },
            {
                "subtitle": "base model ID let&#39;s put it in next line",
                "start": 2910.319,
                "dur": 5.601
            },
            {
                "subtitle": "we have the quantization config as NF4",
                "start": 2913.599,
                "dur": 6.121
            },
            {
                "subtitle": "config and we have the device",
                "start": 2915.92,
                "dur": 7.679
            },
            {
                "subtitle": "map as auto whatever we it&#39;s going to",
                "start": 2919.72,
                "dur": 6.839
            },
            {
                "subtitle": "use CUDA u but it&#39;s fine and you have",
                "start": 2923.599,
                "dur": 7.601
            },
            {
                "subtitle": "the trust remote code as true and you",
                "start": 2926.559,
                "dur": 7.76
            },
            {
                "subtitle": "have the use o token as true because",
                "start": 2931.2,
                "dur": 5.44
            },
            {
                "subtitle": "it&#39;s loading from the model so it&#39;s fine",
                "start": 2934.319,
                "dur": 3.841
            },
            {
                "subtitle": "and loading from the hooking face so",
                "start": 2936.64,
                "dur": 4.76
            },
            {
                "subtitle": "it&#39;s fine o token as",
                "start": 2938.16,
                "dur": 7.52
            },
            {
                "subtitle": "true I think I did some",
                "start": 2941.4,
                "dur": 4.28
            },
            {
                "subtitle": "mistake bits and byes",
                "start": 2946.44,
                "dur": 6.679
            },
            {
                "subtitle": "yep yeah so it loads the base model and",
                "start": 2949.96,
                "dur": 5.879
            },
            {
                "subtitle": "now it&#39;s going to load the tokenizer for",
                "start": 2953.119,
                "dur": 5.161
            },
            {
                "subtitle": "the base",
                "start": 2955.839,
                "dur": 4.641
            },
            {
                "subtitle": "model and this is the tokenizer that",
                "start": 2958.28,
                "dur": 3.24
            },
            {
                "subtitle": "we&#39;re going to use for the fine tune",
                "start": 2960.48,
                "dur": 2.24
            },
            {
                "subtitle": "model as well because we have made no",
                "start": 2961.52,
                "dur": 3.92
            },
            {
                "subtitle": "changes in the tokenizer. tokenizer is",
                "start": 2962.72,
                "dur": 7.359
            },
            {
                "subtitle": "llama tokenizer dot from pre-trained",
                "start": 2965.44,
                "dur": 7.04
            },
            {
                "subtitle": "base model ID use fast is false trust",
                "start": 2970.079,
                "dur": 5.441
            },
            {
                "subtitle": "remote code equal to true and then this",
                "start": 2972.48,
                "dur": 5.2
            },
            {
                "subtitle": "is",
                "start": 2975.52,
                "dur": 2.16
            },
            {
                "subtitle": "fine and",
                "start": 2981.16,
                "dur": 6
            },
            {
                "subtitle": "then we are going to do",
                "start": 2983.559,
                "dur": 7.201
            },
            {
                "subtitle": "is model",
                "start": 2987.16,
                "dur": 6.8
            },
            {
                "subtitle": "fine-tuned equal to eft",
                "start": 2990.76,
                "dur": 5.76
            },
            {
                "subtitle": "model dot",
                "start": 2993.96,
                "dur": 4.8
            },
            {
                "subtitle": "from",
                "start": 2996.52,
                "dur": 4.559
            },
            {
                "subtitle": "pre-trained base",
                "start": 2998.76,
                "dur": 4.24
            },
            {
                "subtitle": "model and",
                "start": 3001.079,
                "dur": 5.161
            },
            {
                "subtitle": "then we have a directory as you know",
                "start": 3003,
                "dur": 5.28
            },
            {
                "subtitle": "fine-tuned",
                "start": 3006.24,
                "dur": 5
            },
            {
                "subtitle": "model/ checkpoint",
                "start": 3008.28,
                "dur": 5.16
            },
            {
                "subtitle": "20s we going to use the last checkpoint",
                "start": 3011.24,
                "dur": 3.879
            },
            {
                "subtitle": "you&#39;re going to use 1500,000 whatever",
                "start": 3013.44,
                "dur": 4.159
            },
            {
                "subtitle": "you have it and that&#39;s all right that&#39;s",
                "start": 3015.119,
                "dur": 4.881
            },
            {
                "subtitle": "up to you what you want to use now we",
                "start": 3017.599,
                "dur": 4.161
            },
            {
                "subtitle": "are going to see something cool now",
                "start": 3020,
                "dur": 3.119
            },
            {
                "subtitle": "we&#39;re going to see the inference",
                "start": 3021.76,
                "dur": 4.28
            },
            {
                "subtitle": "So let&#39;s say user question is going to",
                "start": 3023.119,
                "dur": 7.681
            },
            {
                "subtitle": "be okay there&#39;s some problem",
                "start": 3026.04,
                "dur": 4.76
            },
            {
                "subtitle": "here think there&#39;s some problem",
                "start": 3039,
                "dur": 8.559
            },
            {
                "subtitle": "here what is the problem here",
                "start": 3042.28,
                "dur": 5.279
            },
            {
                "subtitle": "um we did save it here and do we not see",
                "start": 3050.119,
                "dur": 5.561
            },
            {
                "subtitle": "it",
                "start": 3053.44,
                "dur": 2.24
            },
            {
                "subtitle": "here?",
                "start": 3056.599,
                "dur": 5.801
            },
            {
                "subtitle": "Uh should have been dot",
                "start": 3058.44,
                "dur": 3.96
            },
            {
                "subtitle": "slash. Yep, that&#39;s what caused the",
                "start": 3071.24,
                "dur": 5.879
            },
            {
                "subtitle": "problem, I think. Oh, wait. We have the",
                "start": 3074,
                "dur": 5.92
            },
            {
                "subtitle": "fine tuned model here. Oh, that is",
                "start": 3077.119,
                "dur": 4.801
            },
            {
                "subtitle": "because just now made. Got it. Got it.",
                "start": 3079.92,
                "dur": 5.679
            },
            {
                "subtitle": "So let&#39;s see. So what what what was the",
                "start": 3081.92,
                "dur": 7.96
            },
            {
                "subtitle": "error here? You need to provide the bad",
                "start": 3085.599,
                "dur": 8.561
            },
            {
                "subtitle": "token. Yep. Let&#39;s load this. Now let&#39;s",
                "start": 3089.88,
                "dur": 7.36
            },
            {
                "subtitle": "get this and then",
                "start": 3094.16,
                "dur": 5.919
            },
            {
                "subtitle": "this. Yeah. So we have the pad token",
                "start": 3097.24,
                "dur": 4.28
            },
            {
                "subtitle": "here and we have the data collector and",
                "start": 3100.079,
                "dur": 4.401
            },
            {
                "subtitle": "this will going to generate the output",
                "start": 3101.52,
                "dur": 5.64
            },
            {
                "subtitle": "inside this and we have the logs inside",
                "start": 3104.48,
                "dur": 5.2
            },
            {
                "subtitle": "this generating. These are all the logs.",
                "start": 3107.16,
                "dur": 3.8
            },
            {
                "subtitle": "is very important if you want to see",
                "start": 3109.68,
                "dur": 4
            },
            {
                "subtitle": "what went wrong and while this is",
                "start": 3110.96,
                "dur": 4.24
            },
            {
                "subtitle": "happening this is the way we are going",
                "start": 3113.68,
                "dur": 2.84
            },
            {
                "subtitle": "to do the",
                "start": 3115.2,
                "dur": 3.76
            },
            {
                "subtitle": "inference and it was not able to find it",
                "start": 3116.52,
                "dur": 6.2
            },
            {
                "subtitle": "because well it&#39;s not there cool so as",
                "start": 3118.96,
                "dur": 6.08
            },
            {
                "subtitle": "long as it&#39;s running let&#39;s get this",
                "start": 3122.72,
                "dur": 4.879
            },
            {
                "subtitle": "running and this running next to next",
                "start": 3125.04,
                "dur": 4.72
            },
            {
                "subtitle": "and now let&#39;s start typing the output",
                "start": 3127.599,
                "dur": 4.561
            },
            {
                "subtitle": "we&#39;ll see if again error comes then so",
                "start": 3129.76,
                "dur": 4.44
            },
            {
                "subtitle": "when did",
                "start": 3132.16,
                "dur": 4.28
            },
            {
                "subtitle": "y",
                "start": 3134.2,
                "dur": 3.76
            },
            {
                "subtitle": "wildfires",
                "start": 3136.44,
                "dur": 4.119
            },
            {
                "subtitle": "start so This is a very valid question",
                "start": 3137.96,
                "dur": 5.08
            },
            {
                "subtitle": "based on our prompt and it has been able",
                "start": 3140.559,
                "dur": 3.76
            },
            {
                "subtitle": "to has been trained on that. So it",
                "start": 3143.04,
                "dur": 3.84
            },
            {
                "subtitle": "should be able to answer it. So this is",
                "start": 3144.319,
                "dur": 4.681
            },
            {
                "subtitle": "going to",
                "start": 3146.88,
                "dur": 7.199
            },
            {
                "subtitle": "be the evaluate prompt as",
                "start": 3149,
                "dur": 7.88
            },
            {
                "subtitle": "um let me see f question is user",
                "start": 3154.079,
                "dur": 5.601
            },
            {
                "subtitle": "question and then we have we need to",
                "start": 3156.88,
                "dur": 4.76
            },
            {
                "subtitle": "have something",
                "start": 3159.68,
                "dur": 4.01
            },
            {
                "subtitle": "like just",
                "start": 3161.64,
                "dur": 3.76
            },
            {
                "subtitle": "[Music]",
                "start": 3163.69,
                "dur": 4.83
            },
            {
                "subtitle": "answer this",
                "start": 3165.4,
                "dur": 5.28
            },
            {
                "subtitle": "question",
                "start": 3168.52,
                "dur": 5.36
            },
            {
                "subtitle": "accurately and",
                "start": 3170.68,
                "dur": 5.48
            },
            {
                "subtitle": "concisely. That&#39;s it.",
                "start": 3173.88,
                "dur": 3.64
            },
            {
                "subtitle": "We don&#39;t need anything else. It&#39;s the",
                "start": 3176.16,
                "dur": 3.36
            },
            {
                "subtitle": "evaluate prompt. And the next thing is",
                "start": 3177.52,
                "dur": 4.079
            },
            {
                "subtitle": "that the we need to tokenize the prompt",
                "start": 3179.52,
                "dur": 3.68
            },
            {
                "subtitle": "because you remember we tokenize the",
                "start": 3181.599,
                "dur": 3.361
            },
            {
                "subtitle": "data set. So if we tokenize the data",
                "start": 3183.2,
                "dur": 3.399
            },
            {
                "subtitle": "set, we need to tokenize the prompt as",
                "start": 3184.96,
                "dur": 4.639
            },
            {
                "subtitle": "well. Eval prompt and return tensor PD.",
                "start": 3186.599,
                "dur": 4.281
            },
            {
                "subtitle": "Basically, we are going to return the",
                "start": 3189.599,
                "dur": 2.96
            },
            {
                "subtitle": "form of tensor. That is why we do this.",
                "start": 3190.88,
                "dur": 3.679
            },
            {
                "subtitle": "Remember this. Don&#39;t forget about it.",
                "start": 3192.559,
                "dur": 3.601
            },
            {
                "subtitle": "And then we are going to change it to",
                "start": 3194.559,
                "dur": 4.28
            },
            {
                "subtitle": "puda. And then we are going to",
                "start": 3196.16,
                "dur": 8.76
            },
            {
                "subtitle": "do model fine-tuned um sorry fine-tuned",
                "start": 3198.839,
                "dur": 10.28
            },
            {
                "subtitle": "eval. That&#39;s it. And then very simply",
                "start": 3204.92,
                "dur": 7.08
            },
            {
                "subtitle": "torch dot nuggrad. We don&#39;t want to uh",
                "start": 3209.119,
                "dur": 4.801
            },
            {
                "subtitle": "generate the gradients and all. So it&#39;s",
                "start": 3212,
                "dur": 3.52
            },
            {
                "subtitle": "fine. We don&#39;t need the gradient. So",
                "start": 3213.92,
                "dur": 4.24
            },
            {
                "subtitle": "it&#39;s fine to just chuck that out.",
                "start": 3215.52,
                "dur": 4.68
            },
            {
                "subtitle": "Tokenizer. Decode model",
                "start": 3218.16,
                "dur": 4.72
            },
            {
                "subtitle": "fine-tune.generate from tokenized. And",
                "start": 3220.2,
                "dur": 5.639
            },
            {
                "subtitle": "then max new tokens is whatever 100. Uh",
                "start": 3222.88,
                "dur": 5.04
            },
            {
                "subtitle": "let&#39;s not keep it 100. Let&#39;s increase a",
                "start": 3225.839,
                "dur": 4.641
            },
            {
                "subtitle": "little bit. 1024. And then skip special",
                "start": 3227.92,
                "dur": 4.399
            },
            {
                "subtitle": "tokens is true. Special tokens are like",
                "start": 3230.48,
                "dur": 3.839
            },
            {
                "subtitle": "bad and we don&#39;t really need them. So",
                "start": 3232.319,
                "dur": 4.441
            },
            {
                "subtitle": "it&#39;s fine to just, you know, chuck them",
                "start": 3234.319,
                "dur": 6.921
            },
            {
                "subtitle": "out. And then we have the torch. CUDA",
                "start": 3236.76,
                "dur": 6.16
            },
            {
                "subtitle": "dot",
                "start": 3241.24,
                "dur": 4.04
            },
            {
                "subtitle": "empty cache. We don&#39;t want to keep the",
                "start": 3242.92,
                "dur": 4.6
            },
            {
                "subtitle": "cache either. So I&#39;m not sure what",
                "start": 3245.28,
                "dur": 4.88
            },
            {
                "subtitle": "happened. Let&#39;s see. This is done. Now",
                "start": 3247.52,
                "dur": 5.28
            },
            {
                "subtitle": "this is getting",
                "start": 3250.16,
                "dur": 2.64
            },
            {
                "subtitle": "loaded and it&#39;s going to see just see",
                "start": 3253.16,
                "dur": 6.08
            },
            {
                "subtitle": "this and try to understand",
                "start": 3256.4,
                "dur": 6.12
            },
            {
                "subtitle": "this. I&#39;m going to discuss everything in",
                "start": 3259.24,
                "dur": 5.64
            },
            {
                "subtitle": "detail. Yeah, just see this and try to",
                "start": 3262.52,
                "dur": 4.36
            },
            {
                "subtitle": "understand this. So what did we do? We",
                "start": 3264.88,
                "dur": 4
            },
            {
                "subtitle": "needed the base model here because the",
                "start": 3266.88,
                "dur": 5.439
            },
            {
                "subtitle": "fine-tuned model is just a bunch of LOA",
                "start": 3268.88,
                "dur": 5.28
            },
            {
                "subtitle": "configs and these projection vectors",
                "start": 3272.319,
                "dur": 3.441
            },
            {
                "subtitle": "that have been trained. So these",
                "start": 3274.16,
                "dur": 3.439
            },
            {
                "subtitle": "projection vectors need to be sort of",
                "start": 3275.76,
                "dur": 3.92
            },
            {
                "subtitle": "merged with the base model. So what we",
                "start": 3277.599,
                "dur": 3.921
            },
            {
                "subtitle": "are doing here PFT model is basically",
                "start": 3279.68,
                "dur": 3.52
            },
            {
                "subtitle": "from prejen. So we have the base model",
                "start": 3281.52,
                "dur": 3.92
            },
            {
                "subtitle": "and we have the fine-tuned weights and",
                "start": 3283.2,
                "dur": 4
            },
            {
                "subtitle": "then these two are going to be combined",
                "start": 3285.44,
                "dur": 3.679
            },
            {
                "subtitle": "together to create this model fine-tuned",
                "start": 3287.2,
                "dur": 3.119
            },
            {
                "subtitle": "on which we are going to run the",
                "start": 3289.119,
                "dur": 3.521
            },
            {
                "subtitle": "inference. Now suppose if it was we were",
                "start": 3290.319,
                "dur": 4.161
            },
            {
                "subtitle": "not using Lora or we were not using Q",
                "start": 3292.64,
                "dur": 3.84
            },
            {
                "subtitle": "lora we were just doing a normal thing",
                "start": 3294.48,
                "dur": 3.599
            },
            {
                "subtitle": "normal the whole finetuning then we",
                "start": 3296.48,
                "dur": 3.2
            },
            {
                "subtitle": "would not have needed this because PFT",
                "start": 3298.079,
                "dur": 3.121
            },
            {
                "subtitle": "model we would not be using we not using",
                "start": 3299.68,
                "dur": 4.04
            },
            {
                "subtitle": "any quantized version we are not using",
                "start": 3301.2,
                "dur": 4.48
            },
            {
                "subtitle": "anything. So that is why it&#39;s very",
                "start": 3303.72,
                "dur": 3.56
            },
            {
                "subtitle": "important. I hope you understand my",
                "start": 3305.68,
                "dur": 4.08
            },
            {
                "subtitle": "point. So this is the PFT model finetune",
                "start": 3307.28,
                "dur": 4.319
            },
            {
                "subtitle": "model is checkpoint 20 and now this is",
                "start": 3309.76,
                "dur": 4.079
            },
            {
                "subtitle": "the answer. So see there&#39;s the answer",
                "start": 3311.599,
                "dur": 3.921
            },
            {
                "subtitle": "that is coming in because this has been",
                "start": 3313.839,
                "dur": 4.72
            },
            {
                "subtitle": "trained on the data set now and it gets",
                "start": 3315.52,
                "dur": 5.28
            },
            {
                "subtitle": "this start and the common occurrence and",
                "start": 3318.559,
                "dur": 3.601
            },
            {
                "subtitle": "then important to note that they are",
                "start": 3320.8,
                "dur": 3.36
            },
            {
                "subtitle": "common. So this is how we are going to",
                "start": 3322.16,
                "dur": 3.64
            },
            {
                "subtitle": "be able to fine-tune the",
                "start": 3324.16,
                "dur": 4.24
            },
            {
                "subtitle": "data. Sounds good. I think you got the",
                "start": 3325.8,
                "dur": 4.039
            },
            {
                "subtitle": "whole point. So if you have any",
                "start": 3328.4,
                "dur": 3.84
            },
            {
                "subtitle": "questions don&#39;t forget to put them in",
                "start": 3329.839,
                "dur": 4.081
            },
            {
                "subtitle": "the comment section and I&#39;ll get back to",
                "start": 3332.24,
                "dur": 3.359
            },
            {
                "subtitle": "you. I am very active on YouTube and I",
                "start": 3333.92,
                "dur": 3.76
            },
            {
                "subtitle": "try to answer all doubts that you have.",
                "start": 3335.599,
                "dur": 4.72
            },
            {
                "subtitle": "And yeah, like, subscribe, and comment",
                "start": 3337.68,
                "dur": 4.879
            },
            {
                "subtitle": "and share with your friends and all. And",
                "start": 3340.319,
                "dur": 3.841
            },
            {
                "subtitle": "yeah, let&#39;s see you in the next one.",
                "start": 3342.559,
                "dur": 5.081
            },
            {
                "subtitle": "Take care. Bye.",
                "start": 3344.16,
                "dur": 3.48
            }
        ],
        "transcriptionAsText": "So this tokenized train data set is this. So this is very important. This is how we are going to make the our our own data set which is a bunch of text files random text files into a format that can be trained that can be fine- tuned on the model. So it has the last three layers the gate projection the up position and the down prediction. So gate projection. Hello everyone welcome back to my channel. Today we are diving into something really exciting fine-tuning llama 2 model which is one of the most powerful opensource language models out there. But the cool part is that we going to do it on our personal main data set which is a bunch of bunch of text files that you can find anywhere on the internet or you can generate yourself. We'll be using some clever techniques like LoRa which basically lets us train just a tiny bit of the model's parameters while keeping all its original knowledge. Think of it like teaching uh an expert something without making them forget everything they already know. I'll be using the Hawaiian wildfire data set as our example, but the techniques I'll be showing you today can be applied to pretty much any data set you want to work with. Try out on anything you want to work with CSV, be it tables, be it Excel files, be it PowerPoint presentations, be it word documents, whatever you want to, you can do it. And we'll be using 4bit quantization to preserve the model's memory footprint and you can run it in collab very very easily. Okay, since I don't want to waste your time, here's the agenda for today. So let's dive right in. Okay, so the first thing is that we are going to download all the install all the data sets. So the first thing is that we're going to install all the libraries that we'll need today. So let's do that. And also I would like to say before we start just connect here and to change runtime type to make it T4 GPU because we're going to need GPU for finetuning definitely. Um yeah let's do reconnect. Sounds good. Now the next thing is let's install all the libraries that we'll need today. We're going to need PFT. We are going to need accelerate. And I'll tell you why we'll need each of them. Just give me a moment. We'll need um bits and byes. We are going to need transformers and we are going to need data sets. Okay. So let's let's discuss why we'll need each of these libraries in detail while these are loading. So PFT what is PFT? PFT is short form for parameter efficient finetuning. So why is it needed? Because it will be needed to implement the lower or the low rank adaptation. So what is that? Basically, it allows us to fine-tune large models like llama 2 which has so many billions of parameters without updating all the parameters. And basically, we'll use Lora config and get PFG model parameters using the PFT which will help us to fine-tune just a few lurank matrices of this entire um 7 billion or 3 billion parameters that we have in total. I I'll get into more detail as we're using the function. So, hold your horses. Next one is pip install aselate. So why do we need accelerate? Because for optimization and hardware utilization. These are all very important because you are actually you have to remember that you are training a billion parameter model. Remember it's a billion parameter model on these like GPUs and uh across different kinds of implementations on your laptop or on Google Collab whatever you're doing. So you need to be as much efficient as possible because you don't want to waste time. You don't want to lose memory. You want to save everything. You don't want to spend so much money without getting enough you know back. That is why it's very important. So this accelerated helps to distribute training across GPUs and it implements sort of like a precision training. So basically it speeds up the processing significantly. You don't need to know so much details but overall a basic idea would be good. Next thing is the pip install bits bits and byes. So bits and byes is going to be sort of like the four bit quantization. we going to use that. Okay. So the next thing is the transformers. So definitely transformers is the main library for transformer models. We're going to import auto tokenizer. We're going to import auto model for causal LM and we're going to use llama tokenizer trainer model which are which is basically used for SFT training. We're going to do SF training which is basically supervised fine tuning. What do I mean by that? So it basically means that you're going to have a data set where you know what you're going to give the output as. For example, let's say we have an instruct model um like a chat model. We know a question, we know an answer. So we're going to provide data set in the form of a question answer format, Q&amp;A format or we're going to provide it like a text document format and then convert it what we're going to do today. So, we're going to give it a bunch of text documents and these are all on like Hawaiian wildfire what I found on the internet like randomly but you can use it for anything you want and maybe some kind of bot some kind of agents some kind of I don't know like some kind of HR questions interview questions whatever like text documents and you can make a model which is basically fine-tuned on that task so that it can answer on the documents text documents perfectly. So that is what is the purpose of today's video and this is what you're going to use it as these training this trainer model can be found from the transformers library. Cool. Now next thing comes the data set. This is very simple. We're going to use the load data set model and we're going to do kind of data set efficient loading and processing pre-processing of data set to make it in the way we want for our model to train on. So these packages they work together to enable the efficient fine-tuning of large language models on basically limited hardware resources. uh which is exactly what our code is doing because we don't have that much many resources many GPUs and everything we have limited resources so we going to make the maximum use out of them great so this is done so now next thing we're going to need is pip install GPU util so what does this do so this basically does something like the GPU utilization you will be able to suppose you're not using collab because in collab we have only one GPU which is a T4 GPU but suppose you are using your college servers you're using somewhat like you have borrowed some kind of machines from your friend from your company you're working on the machines in your company so anywhere you do this it's very important to you know see what resources you have how many GPUs you have at your hand and how many you are able to use what's the memory distribution you can do among the GPUs cool so let's see so import torch now we're going to have import GPU PU util um now we're going to have import OS which is our um operating system and now we're going to do GPU util dot show just remember this code this is very important because this will help you forever to understand how many resources you have at your hand how many you know what to use and whether someone else is using it or not whether there's an condition or not whether there's some issues some thread running demon thread which is the main thread that runs so there's many problems here so basically this gives you an idea of what you're dealing with, how much resources you have and what to what extent you can push your limits. So torch dot CUDA is available. Yeah. So torch.ca is available and we going to do print GPU is available and otherwise GPU not available. That's it. So this is the thing and then now you're going to do is OS dot environ uhment variable. So this basically sets the environment variable to what we want. So CUDA device order this is going to be PCI bus ID. I I'm going to explain what this means. But let me just complete writing this visible devices is going to be zero because there's only one device that's visible. Okay. So let's come back to this um PCI bus ID actually. So this basically what does it mean? So basically our CUDA basically provides two main methods for ordering or numbering the GPUs. So there can many many number of GPUs, right? This is just a general example because it has only one. You could have skipped that. But if you have many GPUs, it basically PCI bus ID numbers GPUs based on their PCI bus locations. So basically every GPU is connected to our computer's motherboard through the PCI. Um you don't need to understand in this detail. So basically every GPU is connected to your mother's oh so basically every GPU is connected to your computer's motherboard through the PCI bus. So you don't need to understand this in so much detail but it's sort of like a connection that you have and it's used for ordering or numbering the GPUs. So this can help you when you have multiple GPUs but otherwise it's fine. Um that's it. So let's move on. So let's run this and let's move on. So you will see what the resources look like when you're doing this. See GPU is available and we have no usage of GPU right at the moment but GPU is available and we can use it. Great. Sounds good. So now let's move on to the next thing. So we have import torch. We have import transformers. Oh, we've already imported torch but that's okay. It doesn't matter. Uh from transformers we're going to import auto tokenizer. and auto model for um okay coer lm and we going to import bits and byes config well I did not want to autocomplete everything but it doesn't matter we are going to import llama tokenizer and uh from hugging face next we are going to import from hugging face because we'll need the model from there and also yeah data set is ours but model we'll need from there from hugging face hub import notebook login you will need to login to the notebook because llama model that we're going to use is actually it needs to be given access to so basically you will get access don't worry about it but there are a few steps I'll show you all the steps so don't worry about it but it will take some time to get the access so yeah for me it took around 15 minutes but for you might be longer but yeah it uh you will get it within definitely few hours. So I'll come back to it. Wait. Um now next thing is from PFT import prepare model for K bit training for 4bit quization. I I'll come in detail but this is what we need. And then we are going to need Lora config. We're going to use Lora for the 4bit quantization. And then get PFT model. And that's it. And then from that's okay. We already have everything. Yeah. So let's move on. So if we are using collab GPU, so which we are. So just give a short notice this one to say that if you're using your computer's GPU or something else then this will not be um activated. So yeah, so this is just to make your lives easier and import output. We're going to be able to see the output properly. So that is why we're doing this. Um this actually enhances the output format a lot. output dot enable custom widget manager. Great. Sounds good. Now we're going to do is Now we're going to do the notebook login. Collab GPU in OS dot environment. We going to do not equal to again past um hugging base cli login and then else we're going to do notebook login. Let's see. We're going to do the hugging page login. Exactly. So this is the token. Go in here. Yeah. This is what your token. You can do yes, no. It's up to you. And now login is valid. It has been shared. And now the current active login is login. Great. Now you have logged in. So it now knows which models you have access to. Yeah. Now you can see right here. So these are the gated repositories that I have access to and I have asked for the access to these models. So let me show you. So this is the model that we are going to use today. Metal lama 27B chat. Right? So for this model you need to get access. So if you see here you have been granted access to this model. So otherwise what you will see is you will see a list of something. Let me see if I have if I have access to this. If I don't have access to this yeah see you need to share contact information with meta to access the model. So this is what you will see when you are doing this chat thing which I have already been granted access because I've run the code before. So this one you'll have to review and then you have to give your first name, last name, date of birth, country, affiliation. It can be your college. It can be your school. It can be the company you're working in. It can be anything. It doesn't really matter. And then just click submit and submit. That's it. You will get it within 15 minutes. You will see it appearing here. These are the models for which you have added asked access for. So, as you can see, this one I haven't yet got access. I asked on October 4th. But yeah, that's okay. I don't really need it anymore. But this one I needed. So, I got it really fast. So, it will get accepted easily, I'm telling you. So, just don't worry about it. and we can use it. Sounds good. Now just you need to wait for a few minutes. Just wait here. That's it. Don't worry about it. Okay. So the next thing that comes is base model ID. So now we going to provide the base model ID. So because on which we are going to do the finetuning. This is going to be metal lama llama 27B chat HF. this one which I just now said for this you will need access so just get the access don't worry about it bnb config this is going to be the config in which we're going to load the model because we're going to do it in 4bit quantization so that's it bits and byes config and then it's going to be it's going to be simple from here yeah let's see load in 4 bit load in for bit is true bnb 4bit whose content double quant is true and then we have four bit quant type NF4 and this is the compute D type which is B floor 16 what does it mean let me come in detail it's okay so four bit quantization you understand we're going to load in 4 bit that is why so this basically means that we are going to do double quantization because it's basically saves memory so much so we were loading in 4 bit and then we are quantizing it again on top of it so that is what is called double quantization and this is because it will make it so much smaller without much loss in accuracy. We are not really looking for accuracy right now. But if you don't want to do this, you can you can do it too. You can just remove this line, comment out this line. It's up to you. And this is the quant type. So this is just a like a quant type that we follow generally. So it basically um uses this normal float 4 and NF4 is optimized for normal distributions in neural networks. So it basically preserves model accuracy as well. So we don't want to because we're doing double quantization. This will lead to a big hit in the accuracy. But we are preserving that using NF4. So that's it. And this is just B floor 16. This is the generally the data type that we generally use for fine-tuning and for loading. So B floor 16, remember this B floor 16. We're going to use it forever. Don't use floor 16 normal. And don't use FP32. Don't use anything else, but use B floor 16. Sounds good. Now let's load the model. Auto model for causal LM. Um, sorry. dot from pre-trained. Okay. Auto model from causal LM pro dot from pre-trained. We are going to do the base model ID that we have already determined uh written here. And then quantization config would be equal to BNB config. That's it. I don't need anything else. It's more than enough. BNB config. That's it. So quantation config is going to be PNB config. Let's load the model. Okay. I think I missed something. Let me see. Llama 27B. Let me just copy this. And it's a chat. It should have been chat. Yeah. Sorry. Sorry. Um, wait a second. Let me check. Oh, right. Right. HF. We need the hugging face type. HF is something we'll need otherwise it will not work. This is something. This is a bit different model. There are a lot of variations. Yeah, see now it's working. So, this model we are going to load and we're going to fine-tune this model on our personal data set which is a bunch of text files. Cool. As long as this is waiting. So let's start writing the next part of the code. We don't want to waste time. Get clone. Let's see. I'll clone the repository here which has a bunch of text files. So yeah, github.com club and then find I could find the easiest example bunch of text documents online. So that is why I use this data set. You can use anything. I'm telling you you can use anything. This is the most simple thing ever possible. Let's see. Let me show you what the Git repository looks like. We have data. See, there's just like a text document, normal text document talking about the Hawaiian wildfire. The first text document, the second text document, nothing. Nothing like this. Just a bunch of text documents. Simple written. No formatting, no bullet points, nothing. Literally no tokens, anything. Just get whatever you want. It can be CSV file. I'm telling you. It can be CSV file, word document, PDF documents. It can be word documents. It can be Excel, Excel sheets, presentations, anything you want. All is fine. So this is what we going to use today. This is going on. So this is the get clone that we are going to do. And we're going to put it here. We're going to bring the data set out because we don't want to put it inside this data repository because we're going to use the like the home repository. So let's do that as the next step. Yeah, we can do that when this cloning is done. But for now, let's write the next part of the code. So, train data set is going to be load data set. Yeah, load data set. And then this is going to be text. We're going to load text data set. And data files is going to be train and this is going to be a list. So list is going to be what? Content slash well it's correct but it's not too much correct. Finetuning llm/data/i Hawaii wf1.tx txt. This is the first one and like this we are having a lot of data. So how many data do we have? We have 10. We have 10. Yeah, 10. So let me just copy this out 10 times. Or we can just use two for now. I I mean it's just for example purposes. So you can put in any number of data you want and that's up to you. It's fine. Okay. Let's let's keep two only. Um I don't want to make it too much complicated or take I don't want it to take too much time because again this is because of training purposes sample purposes. Split train. This sounds good. Now the train data set is coming in. Now the next thing that comes in is yeah cloning is done. The model loading is done. Now yeah I think because this has not been brought out. Let me see. Yeah, we have here and we have in data. Now we are going to bring out bring it out outside. Are we are we have been able to um Okay, it doesn't really matter. It's inside this, right? So, it's already in the I think I lost something. Whatever. I don't want to waste too much time here. So, let's just change it to three. Let's just change it to three and let's just change it to two and three. It is already there. So, what's the issue? Train data files equal to train. Ah, got it. So this is going to be a dictionary. Cool. So this is going to be a dictionary, right? Um okay. I think this is a dictionary and this is going to be I'm not very wrong. And how I A Y WF3 we have that. So why is it saying that? Is it a spelling mistake? WF3 content/finetuning finetuning llm/data/hawaii wf3. Okay, I am not able to get why this is showing an error. YWF maybe I can make it four or something. Unable to find fine tuning. Ah, see this kind of errors make your life so much difficult. Great. Now it works. So we have the train split. Now the next thing we are going to do is train data set text zero. Let's see an example. So the in early morning of 9 coordinated the transaction. Let me see which one did I take finally. It's the fourth one. See this is the text coordinate transportation for v who. So this is how this data set has been created. So this text file has been broken down into lines and each line constitutes a part of the data set. Let me if you are not still believing me let's see one see had taken refuge the entire thing is there in the second line second element of the data set. So that's it. Now let's move on to the next one. So now we're going to do is tokenizer. So what is the step basically? So basically after getting the data set our next step is always to get the tokenizer and to tokenize the text document the data set. So that next step is setting up the training arguments and all and then choosing the correct library to train and then start the training and the next thing is inference and then we are done. It's very simple just just pay focus and then literally my fine tuning is the next startup business in the next few years. So I mean if you are really good at finetuning you can start your own startup as well. Let's see what what you do with this information. So from pre-trained base model ID and used past equal to false and then trust remote code equal to true because you're using hugging face library. So you need to tell the Google collab that it can able to it's able to trust the hugging face code and you are going to add end of sequence token as true because you will need the token ending for why would we need the token ending? Well because because it helps to mark the end of the text sequences and it generates wellformed outputs. So use fast equal to false. Why are we doing that? Because it ensures consistent tokenization during finetuning. Otherwise fast tokenizer is the rust based fast tokenizer um but it it is not very like efficient um or accurate. So more reliable for llama model is basically during finetuning to disable the fast one slower but it prevents potential tokenization inconsistencies and the US token is very important. So this will this will be really good. Okay. So now this part is very very very important. Pay close attention. Okay. Okay. So this thing is very important what I'm going to talk about here. So this is basically specific to llama models. So what do I mean by that? So basically here if the pad token is none. So llama models don't actually have the pad token. So that is why it's very important to provide the pad token to the tokenizer otherwise llama models won't work on like uneven data sets. So it doesn't have any pad token. So just use end of sequence token as the patch token. So what we're going to do is tokenizer dot add special token token as pad token and then this pad token would be tokenizer end of sequence token eos token. That's it. You're loading the tokenizer and you are going to provide the bad token as the end of sequence token. Now we going to make the tokenize. This is very important. This is how we are going to pre-process the data set which is our personal data. I think there was some timeout issue. Let me try it again. Um okay. So these things now they add special token I am sorry. Yeah done. So these things you need to keep in mind because I mean you have chat GPT you have claw sign but still just keep in mind when you're doing your interviews or talking about an exams whatever. So this tokenized train data set is this. So this is very important. This is how we are going to make the our our own data set which is a bunch of text files random text files into a format that can be trained that can be fine- tuned on the model. So for phrase in train data set it's going to be tokenized train data set dot append tokenizer phrase text. Okay. So what do I mean by this? So basically in the train data set we have the different kinds of phrases. Right. So this is one phrase in the train data set. See this is a phrase right? This is not a complete sentence is a phrase and tokenized train data set will append the tokenized version of this text. So okay so let's see an example how this looks like. So let's see an example. Let's first run this and then let's see tokenized train data set one. See, so this is a tokenized train data set that our model is able to understand because it needs to be in a format that our model is able to understand. Otherwise, it's impossible for the model to basically convert random text files into something to fine-tune. So look at this. So this is basically all the different ids. This has been broken down into individual tokens. It can be had taken refuge or it can be had taken as one token, refugee second token in the as a third token and whatever it has its own vocabulary and based on that it is able to convert this into the input ID. So these are all the different input ids and this is the attention mask. So what do I mean by the attention mask? So this attention mask means that this um input ID specifically um input ID is basically a token. Okay. So don't don't worry about it. Don't think it's something else. So this is basically the token that will be used and this is the numerical representation of the text. So hat taken is represented as one. Refuge is represented as 750 based on the vocabulary of this model and this tokenizer. And then the attention mask is basically all at are one because it has not been padded. So suppose there's a token that has been padded. Let's see an example. You will see the pad token at the end and in that case this is also not padded. But if you if you're padding a basically data set or a line then the attention mask will all be zero after that and it will be always a pad token. So we don't know the pad token exactly what the token is. It's the EOS token. So let let's see let's see what it looks like. EOS token do do like your own experiments because otherwise it's very difficult for you to understand everything based on like listening knowledge or theory. This is the best way to learn. So tokenizer us token is this. So this is the pat token as well. So this here you will see all this based on numerical and then the tension mask. Sounds good. Now we are going to do the model gradient checkpoint. Now all the part comes where we're going to train the model because we have loaded the uh tokenizer and we have converted our model uh our data set into tokens and then we're all ready to go. We have the data set, we have the tokenizer. Now we just need the model. So this is going to be checkpointing dot enable and then model is going to be prepare model for kbit training model. Now it's going to be config equal to lura config. So lura config is going to be r equal to0. I'm going to talk about every of these um parameters in detail. So let me just complete it. Loa alpha is going to be not zero but 64 and then the target models are going to be q proj um gate projection. I I'll I'll discuss it. I'll discuss this in great detail. So so that you can able to understand this wait um approach and then opro yeah we have total seven and then bias um bias is going to be none bias is going to be none and then load dropout is going to be 0.05 05 and then the task type is going to be causal lm. So the auto complete is not really good here but yeah so this is the get pfg model and then model and then config. Okay, sounds good. I think I did some issue model gradient checkpointing dot enable. Okay. So again I would suggest you to just use explain error here in Gemini or you can use it on chat GPT whatever you want but it's all okay. So here the issue I know is that it's going to be checkpoint. Yeah this you can do either or you can do checkpointing and both works. and then um okay sorry this is not zero this should be eight okay so let's dive into the details of each and every parameter here um okay okay so r equal to0 r equal to 8 so r equal to 8 is a good balance for most tasks because a lower r means that it's going to be less expressive and basically r equal to 8 means update matrices are rank eight so what do I mean by that so let let Let us see the transformer example. So then we'll be able to understand better. So the highle transformer block structure is this. So here you have the input text and then you have the tension block and inside the tension block you have two layers. So you have the multi head attention and then which feeds the output into the feed forward network or FFN. So let's let's dive into the multi head attention now. So this is the multi head attention detail. So you have the input, you have the linear layers, now we have the query, key and value. So these are the ones that are targeted because it's very important. And next is the attention scores. And then it now goes through the O projection which is the output projection. So this is also targeted because these are the like it targets basically the update or rank matrices of specific layers which are the most important. So what do we mean by the query projection layer? So let's discuss that. So query projection layer is basically the projects the input into the question space. So for example let's say we have what happened in Hawaii. It becomes a query vector. So now the original is going to be suppose 768 + 768 matrix and with Laura it's going to be 2 768 + 8 and 8 + 768 matrix because we have R equal to 8 is the rank. So this way this is broken down into smaller matrices because large matrices are very difficult to deal with. So this breaks it down to smaller matrices. Try to understand this is very very important and this will clear everything you have doubt in. Now the key projection or K projection. So this projects the input to answer lookup space. So what do I mean by that? So basically it transforms the text into searchable keys. So basically suppose we have what happens in Hawaii. Now it knows that Hawaii is connected to this this this this. So this is like a key key value pair right? So you have Hawaii and Hawaii is project projected to the wildfire. It has projected to the forest whatever it is. So you have a key lookup pair. So and the next thing is the value projection. So the value projection is basically it projects the input to the answer space and it transforms the text into actual information that will help us. So let so let's discuss let's summarize. So first thing is query projection. queries projects the query what happened in Hawaii what is a wildfire and all this into a kind of token that is able that that helps us to understand it that helps the computer to understand it next thing is the K projection or the key projection it transforms the text into searchable keys and third thing is the V projection or value projection which transforms the actual gives me the answer to the problem and now the output projection it basically combines and transforms and filters the output such a way that we are able to understand in our own language Next thing is the feed forward network. So it has the last three layers the gate projection the up projection and the down projection. So gate projection it controls the information flow acts like a smart filter. There are guardrails there are this not to be said there are some kind of passwords the these are suppose these are not to be shared at all. So this is the gate projection. Next thing is the up projection which means expands the information and we also need a down projection because we are able to control these parameters based on what our data set looks like. This is very important. And this basically clears everything that you have doubt in because we have up projection, gate projection, upward projection, downward projection. We have um Q projection, query projection, key projection and value projection. We have an understanding of every one of these seven projections. That's it basically. That's it. That that is enough for your interview and everything that you need to know even for research purposes as well. This is enough and okay. So, okay. So, great. So, now we have the PFT model. We know what everything does here. And now we are going to train the model. Let's do that. Transformers uh sorry, trainer equal to transformers dot trainer. And this is going to be model equal to model. And then it's going to be train data set equal to organized train data set. is going to be args equal to transformers dot training arguments and then we need the output directory as dot /finetuned model. Now it's going to be per device train size as let's say it's two to make it faster. This is a bad size like if it's larger size then 2 to22 will be 222 will be done in each iteration otherwise it's yeah whatever it can be one. And next thing is num train epo total epox total iterations we want to run it for total we want to run it for three we have learning rate as 1 e - 4 10 ^ - 4 minus 4 and we have max steps max steps I don't want to run it more than 20 but listen to me you will be running it for at least 1500 steps Just let it be there, run there for some time because I want to finish it fast. That is why I'm doing it 20. But you need to do it at least for,000 steps or 1500 steps to actually see valuable things coming out of it. Okay. So now next thing is BF16 is going to be false. Here we're not going to use BF16. I mean you can use BF16 but we want to see what happens without BF16 as well because we're already doing double quantization before as I showed you. remember um yeah already using double quantization so it's fine at the end to not use PF16 to use a standard FP32 now optim is going to be the this is a bit this is a new thing so I'm going to discuss this a little bit 8 bit something you can use or you cannot use whatever whatever it's fine and now it's coming to be the login directory to be the log and you have the save strategy as epoch and you have the save steps as 50. It's fine. If you're doing for 1500 or 2,000 data points, then it's very important to do this to 50 because you want to see what's going on, whether loss is decreasing, loss is increasing, whether you need to change any kind of other hyperparameters or not. But yeah and then you have this and then the next thing is you need the data collector. So this is something I'll need to explain to you in greater detail. So oh be ready. So I'll just finish this and then we will discuss data collage model link as tokenizer MLM equal to false and then we have the normal thing like model config dot use cache equal to false and And then we have the trainer train. So I'm going to let it running and I'm going to explain these all these in great detail and I did a spelling mistake. Okay, sounds good. Um, okay. Model config is not defined. Um, sorry, it's going to be model.config. That's all right. Okay, now let's discuss let's discuss all this in detail. Okay. So definitely the model you can understand then the data set you can understand then the bat size you know two and gradient accumulation steps is two. So remember this gradient accumulation step is basically batch processing flow. So it's basically the how we are processing the entire data set. So basically in the iteration one we're going to do a forward pass and we're going to have the gradient calculated and stored and the second iteration we're going to do again a forward pass and then we have the again the gradient calculated and then these two gradients we're going to combine and then do the model update. We're not going to do it in between. So this is what is called the gradient accumulation steps is two. So it's going to remember the gradient that calculated. It's going to combine the two gradients to conduct specific formula and then it's going to update the model. So this is the gradient accumulation steps. You can change as you want it but it's better to keep it as two because it's the most standard and then you can you can play with it. I would encourage you to play with it because your interview mat might ask you to do this to do that and it's very important that you have a good knowledge of this because this is our future. And then comes the number train box is three runs through the entire data set three times and next is max steps is 20. So it stops after 20 update steps and then you have the learning rate at 10 the minus 4 which means a small learning rate for finetuning is fine. You don't want to overfit and you don't want to um you want it to be studied incrementally and bloistine is not being used that's okay. Now this is very important optimum w optimizer is paged Adamw 8 bit. So this is an memory efficient Adamw optimizer. So standard Adamw means that all the parameters and optimizer states are in memory. But paged 8 bit adamw means that model parameters are basically like paged optimizer states they are 8 bit quantized states that are in memory. It's not the entire thing is in memory. We don't want the entire thing because anyway we're not training it. We're not loading it in the full memory and it's not possible not needed. So you don't want to waste things. So basically this optimizer is for the 8 bit quantized states and the rest is very simple to understand. Now comes the data coller which is very important. And the data collator is basically what it does is it has like an input sequence. So sequence one has a lot of tokens. Total tokens is suppose it's 50 and sequence 2 has a total tokens as 30. So what collator does is finds the max length which is 50. Basically the padding thing is done by the data collator which is very very very important. I cannot stress it enough. This is important to have a optimized data set which is of the same length of the same width. not same length of the same width throughout so that it's able to learn everything perfectly. So the same max length is going to be 50 and now we're going to do pad the shorter sequences. Basically I showed you the input tokens are going to be comma and then pad pad pad token whatever it is and then the attention mask is going to be 00 0 because it's the padding thing we don't want it to learn but we want the data set to be of the same width. I hope you understand this. So this is the final complete training flow. Remember this very well. So data flow during training first raw text how fires next thing is tokenization next thing is data collision next thing is patching then comes forward pass then is the calculate loss and then is the accumulate gradients and then update weights and then save checkpoints. That's it. You have the model and then inference. Very very simple and very very like great. Okay. I will need to provide an API key. Okay, I'm going to blur this out but you just I have got my API key. It's absolutely free. It helps you to keep a track of what weights are there, how much you have trained, what is the current status and everything. Great. I think we have everything here. Now we just need the inference and then we are done. So let's start writing the inference as we are as calculating the loss. So I'm going to write in a separate way because we want we often have like in Python files and we are doing it then we have it in different files we have a train.py and we have a test.py by standard like nomen cle so that is why I'm making the import statements again so we are going to do import torch and then from transformers import auto tokenizer and auto model for causal lm next thing is from transformers import bits and byte It's config llama tokenizer. Understand this inference thing as well because if you train a model and it's perfect, it is great but you don't know how to inference it then just throw it in the dust bin from PFT import PFT model. Next thing that comes is this model ID is going to be this is done. So obviously the losses decrease but it can go a lot less because I stopped it at 20. Just do it till 1,000 and you will see the difference guaranteed. Base model ID is going to be the same thing and I'm not going to write it again. So this is the thing. I'm going to share all the links in the description so don't worry about it. And now it comes the NF4 config normal float 4 config cuz we're doing it in 4bit quantization that is why bitsson bytes config now we have load in I really don't think we need to write this again let's copy paste and we have it There you go. We have load in 4 bit as four. We have BNB 4 bit use double quant is true. We have BNB 4 bit quant type as NF4. And we have the data type as B416. I hope you know this. I've already talked about it. So I think it's okay. I think there's a spelling mistake I have here. So this is very important. Now the next thing is tokenizer equal to llama. You have to do the entire same thing the way you did the prep-processing there. you're going to do it here because that is how the model recognizes the text and that is how the model is able to give the answer. Okay. Okay. Now we have the from pre-trained and you have base model id you have use fast think we can use this let's see huh yes add us token is true do the same thing exactly the same thing that you have done during training if you really want your model to succeed well if you don't then it's fine but I'm just saying that you need use fast is false and then trust remote code as true and then add US token as true now this is just auto completed but yeah we can use it I think so we can use it base model is going to be auto model for causal LM do from preent we have the base model ID let's put it in next line we have the quantization config as NF4 config and we have the device map as auto whatever we it's going to use CUDA u but it's fine and you have the trust remote code as true and you have the use o token as true because it's loading from the model so it's fine and loading from the hooking face so it's fine o token as true I think I did some mistake bits and byes yep yeah so it loads the base model and now it's going to load the tokenizer for the base model and this is the tokenizer that we're going to use for the fine tune model as well because we have made no changes in the tokenizer. tokenizer is llama tokenizer dot from pre-trained base model ID use fast is false trust remote code equal to true and then this is fine and then we are going to do is model fine-tuned equal to eft model dot from pre-trained base model and then we have a directory as you know fine-tuned model/ checkpoint 20s we going to use the last checkpoint you're going to use 1500,000 whatever you have it and that's all right that's up to you what you want to use now we are going to see something cool now we're going to see the inference So let's say user question is going to be okay there's some problem here think there's some problem here what is the problem here um we did save it here and do we not see it here? Uh should have been dot slash. Yep, that's what caused the problem, I think. Oh, wait. We have the fine tuned model here. Oh, that is because just now made. Got it. Got it. So let's see. So what what what was the error here? You need to provide the bad token. Yep. Let's load this. Now let's get this and then this. Yeah. So we have the pad token here and we have the data collector and this will going to generate the output inside this and we have the logs inside this generating. These are all the logs. is very important if you want to see what went wrong and while this is happening this is the way we are going to do the inference and it was not able to find it because well it's not there cool so as long as it's running let's get this running and this running next to next and now let's start typing the output we'll see if again error comes then so when did y wildfires start so This is a very valid question based on our prompt and it has been able to has been trained on that. So it should be able to answer it. So this is going to be the evaluate prompt as um let me see f question is user question and then we have we need to have something like just [Music] answer this question accurately and concisely. That's it. We don't need anything else. It's the evaluate prompt. And the next thing is that the we need to tokenize the prompt because you remember we tokenize the data set. So if we tokenize the data set, we need to tokenize the prompt as well. Eval prompt and return tensor PD. Basically, we are going to return the form of tensor. That is why we do this. Remember this. Don't forget about it. And then we are going to change it to puda. And then we are going to do model fine-tuned um sorry fine-tuned eval. That's it. And then very simply torch dot nuggrad. We don't want to uh generate the gradients and all. So it's fine. We don't need the gradient. So it's fine to just chuck that out. Tokenizer. Decode model fine-tune.generate from tokenized. And then max new tokens is whatever 100. Uh let's not keep it 100. Let's increase a little bit. 1024. And then skip special tokens is true. Special tokens are like bad and we don't really need them. So it's fine to just, you know, chuck them out. And then we have the torch. CUDA dot empty cache. We don't want to keep the cache either. So I'm not sure what happened. Let's see. This is done. Now this is getting loaded and it's going to see just see this and try to understand this. I'm going to discuss everything in detail. Yeah, just see this and try to understand this. So what did we do? We needed the base model here because the fine-tuned model is just a bunch of LOA configs and these projection vectors that have been trained. So these projection vectors need to be sort of merged with the base model. So what we are doing here PFT model is basically from prejen. So we have the base model and we have the fine-tuned weights and then these two are going to be combined together to create this model fine-tuned on which we are going to run the inference. Now suppose if it was we were not using Lora or we were not using Q lora we were just doing a normal thing normal the whole finetuning then we would not have needed this because PFT model we would not be using we not using any quantized version we are not using anything. So that is why it's very important. I hope you understand my point. So this is the PFT model finetune model is checkpoint 20 and now this is the answer. So see there's the answer that is coming in because this has been trained on the data set now and it gets this start and the common occurrence and then important to note that they are common. So this is how we are going to be able to fine-tune the data. Sounds good. I think you got the whole point. So if you have any questions don't forget to put them in the comment section and I'll get back to you. I am very active on YouTube and I try to answer all doubts that you have. And yeah, like, subscribe, and comment and share with your friends and all. And yeah, let's see you in the next one. Take care. Bye."
    }
]